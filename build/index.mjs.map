{"version":3,"sources":["../src/ConversationStore.ts","../src/utils/index.ts","../src/utils/log.ts","../src/types.ts","../src/Tokenizer.ts","../src/utils/request.ts","../src/utils/urls.ts","../src/ChatGPT.ts"],"sourcesContent":["import Keyv from 'keyv'\nimport LRUCache from 'lru-cache'\nimport Tokenizer from './Tokenizer'\nimport { log } from './utils'\n\nimport {\n  TChatGPTHTTPDataMessage,\n  IConversationStoreParams,\n  TCommonMessage,\n  ERole,\n} from './types'\n\n/**\n * conversation manage\n */\nexport default class ConversationStore {\n  #store: Keyv<TCommonMessage, any>\n  #lru: LRUCache<string, TCommonMessage>\n  /**\n   * in case some bad things happen\n   */\n  #maxFindDepth: number\n  #debug: boolean\n  constructor(params: IConversationStoreParams) {\n    const { maxKeys = 100000, maxFindDepth = 20, debug = false } = params\n    this.#lru = new LRUCache<string, TCommonMessage>({\n      max: maxKeys,\n    })\n    this.#store = new Keyv<TCommonMessage, any>({\n      store: this.#lru,\n    })\n    this.#maxFindDepth = maxFindDepth\n    this.#debug = debug\n\n    if (this.#debug) log('ConversationStore params', params)\n  }\n  /**\n   * get message by id\n   * @param id\n   * @returns\n   */\n  async get(id: string): Promise<TCommonMessage | undefined> {\n    return await this.#store.get(id)\n  }\n  /**\n   * set new message\n   * @param msg\n   * @returns\n   */\n  async set(msgs: TCommonMessage[]) {\n    for (const msg of msgs) {\n      await this.#store.set(msg.id, msg)\n    }\n    if (this.#debug) log('lru size', this.#lru.size)\n  }\n  /**\n   * check if the id exists in the store\n   * @param id\n   * @returns\n   */\n  async has(id: string): Promise<boolean> {\n    return await this.#store.has(id)\n  }\n  /**\n   * delete one message\n   * @param id\n   * @returns\n   */\n  async delete(id: string): Promise<boolean> {\n    return await this.#store.delete(id)\n  }\n  /**\n   * clear one conversation，it will be used when you set a new system prompt，which means that you will be in a new context，so early messages will be deleted\n   * @param id last conversation id\n   */\n  async clear1Conversation(id?: string) {\n    let parentMessageId: string | undefined = id\n    let cnt = 0\n    while (parentMessageId && cnt < this.#maxFindDepth) {\n      cnt++\n      const msg: TCommonMessage | undefined = await this.get(parentMessageId)\n      if (msg) {\n        await this.delete(msg.id)\n      }\n      parentMessageId = msg?.parentMessageId\n    }\n  }\n  /**\n   * find messages in a conversation by id\n   * @param id parentMessageId\n   */\n  async findMessages(opts: {\n    id: string | undefined\n    tokenizer: Tokenizer\n    limit: number\n    availableTokens: number\n    ignore: boolean\n  }) {\n    let {\n      id = undefined,\n      tokenizer,\n      limit,\n      availableTokens,\n      ignore = false,\n    } = opts\n    let parentMessageId: string | undefined = id\n    let cnt = 0\n    const messages: TChatGPTHTTPDataMessage[] = []\n    while (parentMessageId && cnt < this.#maxFindDepth) {\n      const msg: TCommonMessage | undefined = await this.#store.get(\n        parentMessageId,\n      )\n      if (msg && !(ignore && msg.role === ERole.assistant)) {\n        let tokensCnt = msg.tokens || tokenizer.getTokenCnt(msg.text)\n        if (tokensCnt <= limit) {\n          if (availableTokens < tokensCnt) break\n          messages.unshift({\n            role: msg.role,\n            content: msg.text,\n          })\n          cnt++\n          availableTokens -= tokensCnt\n        }\n      }\n      parentMessageId = msg?.parentMessageId\n    }\n    if (this.#debug) {\n      log('availableTokens', availableTokens)\n    }\n    return messages\n  }\n  /**\n   * clear the store\n   */\n  async clearAll() {\n    await this.#store.clear()\n  }\n}\n","import { v4 as uuid } from 'uuid'\nimport { stdin, stdout } from 'process'\nimport { createInterface } from 'readline'\nexport * from './log'\n\nexport function getReadLine() {\n  const rl = createInterface({ input: stdin, output: stdout })\n  const iter = rl[Symbol.asyncIterator]()\n  return async () => (await iter.next()).value\n}\n\n/**\n * generate unique id by uuidV4\n */\nexport function genId() {\n  return uuid()\n}\n","export function log(...args: any[]) {\n  console.log('----------------------------------')\n  console.log(...args)\n}\n","import { AxiosRequestConfig } from 'axios'\nimport { TiktokenEmbedding } from '@dqbd/tiktoken'\n/**\n * ChatCompletion 回答\n */\nexport interface IChatCompletion {\n  // {\n  //   \"id\": \"chatcmpl-6psB2OWQOgHwCWzTIsQMnhB3fst1J\",\n  //   \"object\": \"chat.completion\",\n  //   \"created\": 1677821004,\n  //   \"model\": \"gpt-3.5-turbo-0301\",\n  //   \"usage\": {\n  //     \"prompt_tokens\": 47,\n  //     \"completion_tokens\": 391,\n  //     \"total_tokens\": 438\n  //   },\n  //   \"choices\": [\n  //     {\n  //       \"message\": {\n  //         \"role\": \"assistant\",\n  //         \"content\": \"回答\"\n  //       },\n  //       \"finish_reason\": \"stop\",\n  //       \"index\": 0\n  //     }\n  //   ]\n  // }\n\n  id: string\n  object: string\n  created: number\n  model: string\n  usage: {\n    prompt_tokens: number\n    completion_tokens: number\n    total_tokens: number\n  }\n  choices: {\n    // content 即为答案\n    message: { role: string; content: string }\n    finish_reason: string\n    index: number\n  }[]\n}\n\n/**\n * response message\n */\nexport interface IChatGPTResponse {\n  id: string\n  text: string\n  created: number\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\n/**\n * user message\n */\nexport interface IChatGPTUserMessage {\n  id: string\n  text: string\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\n/**\n * system situation message\n */\nexport interface IChatGPTSystemMessage {\n  id: string\n  text: string\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\nexport type TChatGPTHTTPDataMessage = {\n  role: ERole\n  content: string\n}\n\nexport enum ERole {\n  /**\n   * conversation situation\n   */\n  system = 'system',\n  /**\n   * role is user\n   */\n  user = 'user',\n  /**\n   * role is chatgpt\n   */\n  assistant = 'assistant',\n}\n\nexport interface IConversationStoreParams {\n  maxKeys?: number\n  maxFindDepth?: number\n  debug?: boolean\n}\n\nexport interface IChatGPTParams {\n  /**\n   * apiKey, you can get it in https://platform.openai.com/account/api-keys,You can apply for up to 5 at most.\n   */\n  apiKey: string\n  /**\n   * model，default is 'gpt-3.5-turbo'\n   */\n  model?: string\n  /**\n   * print logs\n   */\n  debug?: boolean\n  /**\n   * axios configs\n   */\n  requestConfig?: AxiosRequestConfig\n  /**\n   * configs for store\n   */\n  storeConfig?: {\n    /**\n     * lru max keys, default `100000`\n     */\n    maxKeys?: number\n    /**\n     * Recursively search for historical messages, default `20` messages will be sent to the ChatGPT server\n     */\n    maxFindDepth?: number\n  }\n  tokenizerConfig?: ITokensParams\n  /**\n   * the maximum number of tokens when initiating a request, including prompts and completion. The default value is 4096.\n   */\n  maxTokens?: number\n  /**\n   * The maximum number of tokens for a single message. It is used to prevent from sending too many tokens to the ChatGPT server.\n   * If this number is exceeded, the message will be deleted and not passed on as a prompt to the chatGPT server. The default value is `1000`.\n   * - notice: **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**.\n   */\n  limitTokensInAMessage?: number\n  /**\n   * same reason as `limitTokensInAMessage`, **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**, default value is `false` \n   * - `true`: will ignore ChatGPT server message in the next sendMessage, and will only refer to `limitTokensInAMessage` in history messages\n   * - `false`: will only refer to `limitTokensInAMessage` in history messages\n   */\n  ignoreServerMessagesInPrompt?: boolean\n}\n\n/**\n * Tokenizer params\n */\nexport interface ITokensParams {\n  /**\n   * \"gpt2\" | \"r50k_base\" | \"p50k_base\" | \"p50k_edit\" | \"cl100k_base\", default 'cl100k_base'\n   */\n  encoding?: TiktokenEmbedding\n\n  /**\n   * replace regexp\n   */\n  replaceReg?: RegExp\n  /**\n   * replace function\n   */\n  replaceCallback?: (...args: any[]) => string\n}\n\nexport type TCommonMessage =\n  | IChatGPTResponse\n  | IChatGPTUserMessage\n  | IChatGPTSystemMessage","import { get_encoding, Tiktoken } from '@dqbd/tiktoken'\nimport { ITokensParams, TCommonMessage } from './types'\n\nexport default class Tokenizer {\n  #tokenizer: Tiktoken\n  #replaceReg: RegExp\n  #replaceCallback: (...args: any[]) => string\n  constructor(opts: ITokensParams) {\n    const {\n      encoding = 'cl100k_base',\n      replaceReg = /<\\|endoftext\\|>/g,\n      replaceCallback = (...args: any[]) => '',\n    } = opts\n    this.#tokenizer = get_encoding(encoding)\n    this.#replaceReg = replaceReg\n    this.#replaceCallback = replaceCallback\n  }\n  #encode(text: string): Uint32Array {\n    return this.#tokenizer.encode(text)\n  }\n  /**\n   * get the text tokens count\n   * @param text\n   * @returns\n   */\n  getTokenCnt(msg: TCommonMessage | string){\n    if(typeof msg === 'object' && msg.tokens) return msg.tokens\n    msg = typeof msg === 'object' ? msg.text : msg\n    const text = msg.replace(this.#replaceReg, this.#replaceCallback)\n    return this.#encode(text).length\n  }\n}\n\n// const token = new Tokenizer({\n//   replaceReg: /hello|world/g,\n//   replaceCallback(c: string) {\n//     return 'ok'\n//   },\n// })\n// console.log(token.getTokenCnt('hello world'))\n","import axios from 'axios'\nimport { RawAxiosRequestConfig } from 'axios'\nimport { log } from './index'\n\ninterface IRequestOpts {\n  debug?: boolean\n}\n\nexport async function get(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n  const ins = axios.create({\n    method: 'GET',\n  })\n  if (opts.debug) {\n    ins.interceptors.request.use((config) => {\n      log('axios config', config)\n      return config\n    })\n  }\n  return (await ins({ ...config })).data\n}\nexport async function post(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n  const ins = axios.create({\n    method: 'POST',\n  })\n  if (opts.debug) {\n    ins.interceptors.request.use((config) => {\n      log('axios config', config)\n      return config\n    })\n  }\n  return (await ins({ ...config })).data\n}\n","/**\n * docs https://platform.openai.com/docs/api-reference/chat\n */\nconst urls = {\n  listModels: 'https://api.openai.com/v1/models', // get\n  createCompletion: 'https://api.openai.com/v1/completions', // post\n  createChatCompletion: 'https://api.openai.com/v1/chat/completions' // post\n}\n\nexport default urls\n","import { AxiosRequestConfig } from 'axios'\n\nimport ConversationStore from './ConversationStore'\nimport Tokenizer from './Tokenizer'\nimport {\n  IChatCompletion,\n  IChatGPTResponse,\n  IChatGPTUserMessage,\n  IChatGPTSystemMessage,\n  ERole,\n  TChatGPTHTTPDataMessage,\n  IChatGPTParams,\n} from './types'\nimport { post } from './utils/request'\nimport URLS from './utils/urls'\nimport { genId, log } from './utils'\n\n// https://platform.openai.com/docs/api-reference/chat\n// curl https://api.openai.com/v1/chat/completions \\\n//   -H 'Content-Type: application/json' \\\n//   -H 'Authorization: Bearer YOUR_API_KEY' \\\n//   -d '{\n//   \"model\": \"gpt-3.5-turbo\",\n//   \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n// }'\n\n// role https://platform.openai.com/docs/guides/chat/introduction\nexport class ChatGPT {\n  #apiKey = ''\n  #model = ''\n  #urls = URLS\n  #debug = false\n  #requestConfig: AxiosRequestConfig\n  #store: ConversationStore\n  #tokenizer: Tokenizer\n  #maxTokens: number\n  #limitTokensInAMessage: number\n  #ignoreServerMessagesInPrompt: boolean\n  constructor(opts: IChatGPTParams) {\n    const {\n      apiKey,\n      model = 'gpt-3.5-turbo',\n      debug = false,\n      requestConfig = {},\n      storeConfig = {},\n      tokenizerConfig = {},\n      maxTokens = 4096,\n      limitTokensInAMessage = 1000,\n      ignoreServerMessagesInPrompt = false,\n    } = opts\n\n    this.#apiKey = apiKey\n    this.#model = model\n    this.#debug = debug\n    this.#requestConfig = requestConfig\n    this.#store = new ConversationStore({\n      ...storeConfig,\n      debug: this.#debug,\n    })\n    this.#tokenizer = new Tokenizer(tokenizerConfig)\n    this.#maxTokens = maxTokens\n    this.#limitTokensInAMessage = limitTokensInAMessage\n    this.#ignoreServerMessagesInPrompt = ignoreServerMessagesInPrompt\n  }\n\n  /**\n   * send message to ChatGPT server\n   * @param opts.text new message\n   * @param opts.systemPrompt prompt message\n   * @param opts.parentMessageId\n   */\n  sendMessage(\n    opts:\n      | {\n          text: string\n          systemPrompt?: string\n          parentMessageId?: string\n          onProgress?: (t: string) => void\n        }\n      | string,\n  ) {\n    return new Promise<IChatGPTResponse>(async (resolve, reject) => {\n      opts = typeof opts === 'string' ? { text: opts } : opts\n      let {\n        text,\n        systemPrompt = undefined,\n        parentMessageId = undefined,\n        onProgress = false,\n      } = opts\n      if (systemPrompt) {\n        if (parentMessageId)\n          await this.#store.clear1Conversation(parentMessageId)\n        parentMessageId = undefined\n      }\n      const model = this.#model\n      const userMessage: IChatGPTUserMessage = {\n        id: genId(),\n        text,\n        role: ERole.user,\n        parentMessageId,\n        tokens: this.#tokenizer.getTokenCnt(text),\n      }\n      const messages = await this.#makeConversations(userMessage, systemPrompt)\n      if (this.#debug) {\n        log('messages', messages)\n      }\n      if (onProgress) {\n        try {\n          const stream = await post(\n            {\n              url: this.#urls.createChatCompletion,\n              ...this.#requestConfig,\n              headers: {\n                Authorization: this.#genAuthorization(),\n                'Content-Type': 'application/json',\n                ...{ ...(this.#requestConfig.headers || {}) },\n              },\n              data: {\n                stream: true,\n                model,\n                messages,\n                ...{ ...(this.#requestConfig.data || {}) },\n              },\n              responseType: 'stream',\n            },\n            {\n              debug: this.#debug,\n            },\n          )\n          const response: IChatGPTResponse = {\n            id: genId(),\n            text: '',\n            created: Math.floor(Date.now() / 1000),\n            role: ERole.assistant,\n            parentMessageId: userMessage.id,\n            tokens: 0,\n          }\n          stream.on('data', (buf: any) => {\n            try {\n              const dataArr = buf.toString().split('\\n')\n              let onDataPieceText = ''\n              for (const dataStr of dataArr) {\n                // split 之后的空行，或者结束通知\n                if (\n                  dataStr.indexOf('data: ') !== 0 ||\n                  dataStr === 'data: [DONE]'\n                )\n                  continue\n                const parsedData = JSON.parse(dataStr.slice(6)) // [data: ]\n                const pieceText = parsedData.choices[0].delta.content || ''\n                onDataPieceText += pieceText\n                // if (pieceText === '') {\n                //   console.log('[empty pieceText]', dataStr)\n                // }\n              }\n              if (typeof onProgress === 'function') {\n                onProgress(onDataPieceText)\n              }\n              response.text += onDataPieceText\n            } catch (e) {\n              log('[chatgpt api err]', e)\n            }\n          })\n\n          stream.on('end', () => {\n            response.tokens = this.#tokenizer.getTokenCnt(response.text)\n            resolve(response)\n          })\n        } catch (e) {\n          reject(e)\n        }\n      } else {\n        try {\n          const res = (await post(\n            {\n              url: this.#urls.createChatCompletion,\n              ...this.#requestConfig,\n              headers: {\n                Authorization: this.#genAuthorization(),\n                'Content-Type': 'application/json',\n                ...{ ...(this.#requestConfig.headers || {}) },\n              },\n              data: {\n                model,\n                messages,\n                ...{ ...(this.#requestConfig.data || {}) },\n              },\n            },\n            {\n              debug: this.#debug,\n            },\n          )) as IChatCompletion\n          if (this.#debug) {\n            // log response\n            log(\n              'response',\n              JSON.stringify({\n                ...res,\n                choices: [],\n              }),\n            )\n          }\n          const response: IChatGPTResponse = {\n            id: genId(),\n            text: res?.choices[0]?.message?.content,\n            created: res.created,\n            role: ERole.assistant,\n            parentMessageId: userMessage.id,\n            tokens: res?.usage?.completion_tokens,\n          }\n          const msgsToBeStored = [userMessage, response]\n          if (systemPrompt) {\n            const systemMessage: IChatGPTSystemMessage = {\n              id: genId(),\n              text: systemPrompt,\n              role: ERole.system,\n              tokens: this.#tokenizer.getTokenCnt(systemPrompt),\n            }\n            userMessage.parentMessageId = systemMessage.id\n            msgsToBeStored.unshift(systemMessage)\n          }\n          await this.#store.set(msgsToBeStored)\n          resolve(response)\n        } catch (e) {\n          reject(e)\n        }\n      }\n    })\n  }\n\n  /**\n   * make conversations for http request data.messages\n   */\n  async #makeConversations(userMessage: IChatGPTUserMessage, prompt?: string) {\n    let messages: TChatGPTHTTPDataMessage[] = []\n    let usedTokens = this.#tokenizer.getTokenCnt(userMessage.text)\n    if (prompt) {\n      messages.push({\n        role: ERole.system,\n        content: prompt,\n      })\n    } else {\n      messages = await this.#store.findMessages({\n        id: userMessage.parentMessageId,\n        tokenizer: this.#tokenizer,\n        limit: this.#limitTokensInAMessage,\n        availableTokens: this.#maxTokens - usedTokens,\n        ignore: this.#ignoreServerMessagesInPrompt,\n      })\n    }\n    messages.push({\n      role: ERole.user,\n      content: userMessage.text,\n    })\n    return messages\n  }\n\n  async clear1Conversation(parentMessageId?: string) {\n    return await this.#store.clear1Conversation(parentMessageId)\n  }\n\n  /**\n   * generate HTTP Authorization\n   * @returns\n   */\n  #genAuthorization() {\n    return `Bearer ${this.#apiKey}`\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAAA,OAAO,UAAU;AACjB,OAAO,cAAc;;;ACDrB,SAAS,MAAM,YAAY;;;ACApB,SAAS,OAAO,MAAa;AAClC,UAAQ,IAAI,oCAAoC;AAChD,UAAQ,IAAI,GAAG,IAAI;AACrB;;;ADWO,SAAS,QAAQ;AACtB,SAAO,KAAK;AACd;;;AEiEO,IAAK,QAAL,kBAAKA,WAAL;AAIL,EAAAA,OAAA,YAAS;AAIT,EAAAA,OAAA,UAAO;AAIP,EAAAA,OAAA,eAAY;AAZF,SAAAA;AAAA,GAAA;;;AHjFZ;AAeA,IAAqB,oBAArB,MAAuC;AAAA,EAQrC,YAAY,QAAkC;AAP9C;AACA;AAIA;AAAA;AAAA;AAAA;AACA;AAEE,UAAM,EAAE,UAAU,KAAQ,eAAe,IAAI,QAAQ,MAAM,IAAI;AAC/D,uBAAK,MAAO,IAAI,SAAiC;AAAA,MAC/C,KAAK;AAAA,IACP,CAAC;AACD,uBAAK,QAAS,IAAI,KAA0B;AAAA,MAC1C,OAAO,mBAAK;AAAA,IACd,CAAC;AACD,uBAAK,eAAgB;AACrB,uBAAK,QAAS;AAEd,QAAI,mBAAK;AAAQ,UAAI,4BAA4B,MAAM;AAAA,EACzD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAAiD;AACzD,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,MAAwB;AAChC,eAAW,OAAO,MAAM;AACtB,YAAM,mBAAK,QAAO,IAAI,IAAI,IAAI,GAAG;AAAA,IACnC;AACA,QAAI,mBAAK;AAAQ,UAAI,YAAY,mBAAK,MAAK,IAAI;AAAA,EACjD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAA8B;AACtC,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,OAAO,IAA8B;AACzC,WAAO,MAAM,mBAAK,QAAO,OAAO,EAAE;AAAA,EACpC;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAmB,IAAa;AACpC,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD;AACA,YAAM,MAAkC,MAAM,KAAK,IAAI,eAAe;AACtE,UAAI,KAAK;AACP,cAAM,KAAK,OAAO,IAAI,EAAE;AAAA,MAC1B;AACA,wBAAkB,2BAAK;AAAA,IACzB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,MAMhB;AACD,QAAI;AAAA,MACF,KAAK;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA,SAAS;AAAA,IACX,IAAI;AACJ,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,UAAM,WAAsC,CAAC;AAC7C,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD,YAAM,MAAkC,MAAM,mBAAK,QAAO;AAAA,QACxD;AAAA,MACF;AACA,UAAI,OAAO,EAAE,UAAU,IAAI,uCAA2B;AACpD,YAAI,YAAY,IAAI,UAAU,UAAU,YAAY,IAAI,IAAI;AAC5D,YAAI,aAAa,OAAO;AACtB,cAAI,kBAAkB;AAAW;AACjC,mBAAS,QAAQ;AAAA,YACf,MAAM,IAAI;AAAA,YACV,SAAS,IAAI;AAAA,UACf,CAAC;AACD;AACA,6BAAmB;AAAA,QACrB;AAAA,MACF;AACA,wBAAkB,2BAAK;AAAA,IACzB;AACA,QAAI,mBAAK,SAAQ;AACf,UAAI,mBAAmB,eAAe;AAAA,IACxC;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAIA,MAAM,WAAW;AACf,UAAM,mBAAK,QAAO,MAAM;AAAA,EAC1B;AACF;AAzHE;AACA;AAIA;AACA;;;AItBF,SAAS,oBAA8B;AAAvC;AAGA,IAAqB,YAArB,MAA+B;AAAA,EAI7B,YAAY,MAAqB;AAUjC;AAbA;AACA;AACA;AAEE,UAAM;AAAA,MACJ,WAAW;AAAA,MACX,aAAa;AAAA,MACb,kBAAkB,IAAI,SAAgB;AAAA,IACxC,IAAI;AACJ,uBAAK,YAAa,aAAa,QAAQ;AACvC,uBAAK,aAAc;AACnB,uBAAK,kBAAmB;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,YAAY,KAA6B;AACvC,QAAG,OAAO,QAAQ,YAAY,IAAI;AAAQ,aAAO,IAAI;AACrD,UAAM,OAAO,QAAQ,WAAW,IAAI,OAAO;AAC3C,UAAM,OAAO,IAAI,QAAQ,mBAAK,cAAa,mBAAK,iBAAgB;AAChE,WAAO,sBAAK,oBAAL,WAAa,MAAM;AAAA,EAC5B;AACF;AA3BE;AACA;AACA;AAWA;AAAA,YAAO,SAAC,MAA2B;AACjC,SAAO,mBAAK,YAAW,OAAO,IAAI;AACpC;;;ACnBF,OAAO,WAAW;AAoBlB,eAAsB,KAAK,QAA+B,MAAoB;AAC5E,QAAM,MAAM,MAAM,OAAO;AAAA,IACvB,QAAQ;AAAA,EACV,CAAC;AACD,MAAI,KAAK,OAAO;AACd,QAAI,aAAa,QAAQ,IAAI,CAACC,YAAW;AACvC,UAAI,gBAAgBA,OAAM;AAC1B,aAAOA;AAAA,IACT,CAAC;AAAA,EACH;AACA,UAAQ,MAAM,IAAI,EAAE,GAAG,OAAO,CAAC,GAAG;AACpC;;;AC5BA,IAAM,OAAO;AAAA,EACX,YAAY;AAAA;AAAA,EACZ,kBAAkB;AAAA;AAAA,EAClB,sBAAsB;AAAA;AACxB;AAEA,IAAO,eAAQ;;;ACTf,4BAAAC,SAAA,gBAAAC,SAAAC,aAAA;AA2BO,IAAM,UAAN,MAAc;AAAA,EAWnB,YAAY,MAAsB;AAmMlC;AAAA;AAAA;AAAA,uBAAM;AAgCN;AAAA;AAAA;AAAA;AAAA;AA7OA,gCAAU;AACV,+BAAS;AACT,8BAAQ;AACR,uBAAAF,SAAS;AACT;AACA,uBAAAC,SAAA;AACA,uBAAAC,aAAA;AACA;AACA;AACA;AAEE,UAAM;AAAA,MACJ;AAAA,MACA,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,gBAAgB,CAAC;AAAA,MACjB,cAAc,CAAC;AAAA,MACf,kBAAkB,CAAC;AAAA,MACnB,YAAY;AAAA,MACZ,wBAAwB;AAAA,MACxB,+BAA+B;AAAA,IACjC,IAAI;AAEJ,uBAAK,SAAU;AACf,uBAAK,QAAS;AACd,uBAAKF,SAAS;AACd,uBAAK,gBAAiB;AACtB,uBAAKC,SAAS,IAAI,kBAAkB;AAAA,MAClC,GAAG;AAAA,MACH,OAAO,mBAAKD;AAAA,IACd,CAAC;AACD,uBAAKE,aAAa,IAAI,UAAU,eAAe;AAC/C,uBAAK,YAAa;AAClB,uBAAK,wBAAyB;AAC9B,uBAAK,+BAAgC;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,YACE,MAQA;AACA,WAAO,IAAI,QAA0B,OAAO,SAAS,WAAW;AAjFpE;AAkFM,aAAO,OAAO,SAAS,WAAW,EAAE,MAAM,KAAK,IAAI;AACnD,UAAI;AAAA,QACF;AAAA,QACA,eAAe;AAAA,QACf,kBAAkB;AAAA,QAClB,aAAa;AAAA,MACf,IAAI;AACJ,UAAI,cAAc;AAChB,YAAI;AACF,gBAAM,mBAAKD,SAAO,mBAAmB,eAAe;AACtD,0BAAkB;AAAA,MACpB;AACA,YAAM,QAAQ,mBAAK;AACnB,YAAM,cAAmC;AAAA,QACvC,IAAI,MAAM;AAAA,QACV;AAAA,QACA;AAAA,QACA;AAAA,QACA,QAAQ,mBAAKC,aAAW,YAAY,IAAI;AAAA,MAC1C;AACA,YAAM,WAAW,MAAM,sBAAK,0CAAL,WAAwB,aAAa;AAC5D,UAAI,mBAAKF,UAAQ;AACf,YAAI,YAAY,QAAQ;AAAA,MAC1B;AACA,UAAI,YAAY;AACd,YAAI;AACF,gBAAM,SAAS,MAAM;AAAA,YACnB;AAAA,cACE,KAAK,mBAAK,OAAM;AAAA,cAChB,GAAG,mBAAK;AAAA,cACR,SAAS;AAAA,gBACP,eAAe,sBAAK,wCAAL;AAAA,gBACf,gBAAgB;AAAA,gBAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,cAC9C;AAAA,cACA,MAAM;AAAA,gBACJ,QAAQ;AAAA,gBACR;AAAA,gBACA;AAAA,gBACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,cAC3C;AAAA,cACA,cAAc;AAAA,YAChB;AAAA,YACA;AAAA,cACE,OAAO,mBAAKA;AAAA,YACd;AAAA,UACF;AACA,gBAAM,WAA6B;AAAA,YACjC,IAAI,MAAM;AAAA,YACV,MAAM;AAAA,YACN,SAAS,KAAK,MAAM,KAAK,IAAI,IAAI,GAAI;AAAA,YACrC;AAAA,YACA,iBAAiB,YAAY;AAAA,YAC7B,QAAQ;AAAA,UACV;AACA,iBAAO,GAAG,QAAQ,CAAC,QAAa;AAC9B,gBAAI;AACF,oBAAM,UAAU,IAAI,SAAS,EAAE,MAAM,IAAI;AACzC,kBAAI,kBAAkB;AACtB,yBAAW,WAAW,SAAS;AAE7B,oBACE,QAAQ,QAAQ,QAAQ,MAAM,KAC9B,YAAY;AAEZ;AACF,sBAAM,aAAa,KAAK,MAAM,QAAQ,MAAM,CAAC,CAAC;AAC9C,sBAAM,YAAY,WAAW,QAAQ,CAAC,EAAE,MAAM,WAAW;AACzD,mCAAmB;AAAA,cAIrB;AACA,kBAAI,OAAO,eAAe,YAAY;AACpC,2BAAW,eAAe;AAAA,cAC5B;AACA,uBAAS,QAAQ;AAAA,YACnB,SAAS,GAAP;AACA,kBAAI,qBAAqB,CAAC;AAAA,YAC5B;AAAA,UACF,CAAC;AAED,iBAAO,GAAG,OAAO,MAAM;AACrB,qBAAS,SAAS,mBAAKE,aAAW,YAAY,SAAS,IAAI;AAC3D,oBAAQ,QAAQ;AAAA,UAClB,CAAC;AAAA,QACH,SAAS,GAAP;AACA,iBAAO,CAAC;AAAA,QACV;AAAA,MACF,OAAO;AACL,YAAI;AACF,gBAAM,MAAO,MAAM;AAAA,YACjB;AAAA,cACE,KAAK,mBAAK,OAAM;AAAA,cAChB,GAAG,mBAAK;AAAA,cACR,SAAS;AAAA,gBACP,eAAe,sBAAK,wCAAL;AAAA,gBACf,gBAAgB;AAAA,gBAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,cAC9C;AAAA,cACA,MAAM;AAAA,gBACJ;AAAA,gBACA;AAAA,gBACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,cAC3C;AAAA,YACF;AAAA,YACA;AAAA,cACE,OAAO,mBAAKF;AAAA,YACd;AAAA,UACF;AACA,cAAI,mBAAKA,UAAQ;AAEf;AAAA,cACE;AAAA,cACA,KAAK,UAAU;AAAA,gBACb,GAAG;AAAA,gBACH,SAAS,CAAC;AAAA,cACZ,CAAC;AAAA,YACH;AAAA,UACF;AACA,gBAAM,WAA6B;AAAA,YACjC,IAAI,MAAM;AAAA,YACV,OAAM,sCAAK,QAAQ,OAAb,mBAAiB,YAAjB,mBAA0B;AAAA,YAChC,SAAS,IAAI;AAAA,YACb;AAAA,YACA,iBAAiB,YAAY;AAAA,YAC7B,SAAQ,gCAAK,UAAL,mBAAY;AAAA,UACtB;AACA,gBAAM,iBAAiB,CAAC,aAAa,QAAQ;AAC7C,cAAI,cAAc;AAChB,kBAAM,gBAAuC;AAAA,cAC3C,IAAI,MAAM;AAAA,cACV,MAAM;AAAA,cACN;AAAA,cACA,QAAQ,mBAAKE,aAAW,YAAY,YAAY;AAAA,YAClD;AACA,wBAAY,kBAAkB,cAAc;AAC5C,2BAAe,QAAQ,aAAa;AAAA,UACtC;AACA,gBAAM,mBAAKD,SAAO,IAAI,cAAc;AACpC,kBAAQ,QAAQ;AAAA,QAClB,SAAS,GAAP;AACA,iBAAO,CAAC;AAAA,QACV;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EA6BA,MAAM,mBAAmB,iBAA0B;AACjD,WAAO,MAAM,mBAAKA,SAAO,mBAAmB,eAAe;AAAA,EAC7D;AASF;AAhPE;AACA;AACA;AACAD,UAAA;AACA;AACAC,UAAA;AACAC,cAAA;AACA;AACA;AACA;AAoMM;AAAA,uBAAkB,eAAC,aAAkC,QAAiB;AAC1E,MAAI,WAAsC,CAAC;AAC3C,MAAI,aAAa,mBAAKA,aAAW,YAAY,YAAY,IAAI;AAC7D,MAAI,QAAQ;AACV,aAAS,KAAK;AAAA,MACZ;AAAA,MACA,SAAS;AAAA,IACX,CAAC;AAAA,EACH,OAAO;AACL,eAAW,MAAM,mBAAKD,SAAO,aAAa;AAAA,MACxC,IAAI,YAAY;AAAA,MAChB,WAAW,mBAAKC;AAAA,MAChB,OAAO,mBAAK;AAAA,MACZ,iBAAiB,mBAAK,cAAa;AAAA,MACnC,QAAQ,mBAAK;AAAA,IACf,CAAC;AAAA,EACH;AACA,WAAS,KAAK;AAAA,IACZ;AAAA,IACA,SAAS,YAAY;AAAA,EACvB,CAAC;AACD,SAAO;AACT;AAUA;AAAA,sBAAiB,WAAG;AAClB,SAAO,UAAU,mBAAK;AACxB;","names":["ERole","config","_debug","_store","_tokenizer"]}