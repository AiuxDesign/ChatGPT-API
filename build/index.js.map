{"version":3,"sources":["../src/index.ts","../src/ConversationStore.ts","../src/types.ts","../src/Tokenizer.ts","../src/utils/request.ts","../src/utils/urls.ts","../src/utils/index.ts","../src/utils/log.ts","../src/Chatgpt.ts"],"sourcesContent":["export * from './Chatgpt'\nexport * from './types'","import Keyv from 'keyv'\nimport LRUCache from 'lru-cache'\nimport Tokenizer from './Tokenizer'\n\nimport {\n  IChatGPTHTTPDataMessage,\n  IConversationStoreParams,\n  TCommonMessage,\n  ERole,\n  TLog,\n} from './types'\n\n/**\n * conversation manage\n */\nexport default class ConversationStore {\n  #store: Keyv<TCommonMessage, any>\n  #lru: LRUCache<string, TCommonMessage>\n  /**\n   * in case some bad things happen\n   */\n  #maxFindDepth: number\n  #debug: boolean\n  #log: TLog\n  constructor(params: IConversationStoreParams) {\n    const { maxKeys = 100000, maxFindDepth = 20, debug, log } = params\n    this.#lru = new LRUCache<string, TCommonMessage>({\n      max: maxKeys,\n    })\n    this.#store = new Keyv<TCommonMessage, any>({\n      store: this.#lru,\n    })\n    this.#maxFindDepth = maxFindDepth\n    this.#debug = debug\n    this.#log = log\n\n    if (this.#debug) this.#log('ConversationStore params', params)\n  }\n  /**\n   * get message by id\n   * @param id\n   * @returns\n   */\n  async get(id: string): Promise<TCommonMessage | undefined> {\n    return await this.#store.get(id)\n  }\n  /**\n   * set new message\n   * @param msg\n   * @returns\n   */\n  async set(msgs: TCommonMessage[]) {\n    for (const msg of msgs) {\n      await this.#store.set(msg.id, msg)\n    }\n    if (this.#debug) this.#log('lru size', this.#lru.size)\n  }\n  /**\n   * check if the id exists in the store\n   * @param id\n   * @returns\n   */\n  async has(id: string): Promise<boolean> {\n    return await this.#store.has(id)\n  }\n  /**\n   * delete one message\n   * @param id\n   * @returns\n   */\n  async delete(id: string): Promise<boolean> {\n    return await this.#store.delete(id)\n  }\n  /**\n   * clear one conversation，it will be used when you set a new system prompt，which means that you will be in a new context，so early messages will be deleted\n   * @param id last conversation id\n   */\n  async clear1Conversation(id?: string) {\n    let parentMessageId: string | undefined = id\n    let cnt = 0\n    while (parentMessageId && cnt < this.#maxFindDepth) {\n      cnt++\n      const msg: TCommonMessage | undefined = await this.get(parentMessageId)\n      if (msg) {\n        await this.delete(msg.id)\n      }\n      parentMessageId = msg?.parentMessageId\n    }\n  }\n  /**\n   * find messages in a conversation by id\n   * @param id parentMessageId\n   */\n  async findMessages(opts: {\n    id: string | undefined\n    tokenizer: Tokenizer\n    limit: number\n    availableTokens: number\n    ignore: boolean\n  }) {\n    let {\n      id = undefined,\n      tokenizer,\n      limit,\n      availableTokens,\n      ignore = false,\n    } = opts\n    if (this.#debug) {\n      this.#log('[ConversationStore findMessages start]', id)\n    }\n    let parentMessageId: string | undefined = id\n    let cnt = 0\n    const messages: IChatGPTHTTPDataMessage[] = []\n    while (parentMessageId && cnt < this.#maxFindDepth) {\n      const msg: TCommonMessage | undefined = await this.#store.get(\n        parentMessageId,\n      )\n      if (msg && !(ignore && msg.role === ERole.assistant)) {\n        let tokensCnt = msg.tokens || tokenizer.getTokenCnt(msg.text)\n        if (tokensCnt <= limit) {\n          if (availableTokens < tokensCnt) break\n          messages.unshift({\n            role: msg.role,\n            content: msg.text,\n          })\n          cnt++\n          availableTokens -= tokensCnt\n        }\n      }\n      parentMessageId = msg?.parentMessageId\n    }\n    if (this.#debug) {\n      this.#log('availableTokens', availableTokens)\n      this.#log('[ConversationStore findMessages end]', messages)\n    }\n    return messages\n  }\n\n  async getMessages(opts: { id: string; maxDepth?: number }) {\n    let { id = undefined, maxDepth = 30 } = opts\n    if (this.#debug) {\n      this.#log('[ConversationStore getMessages start]', { id, maxDepth })\n    }\n    let parentMessageId: string | undefined = id\n    let cnt = 0\n    const messages: TCommonMessage[] = []\n    while (parentMessageId && cnt < this.#maxFindDepth) {\n      const msg: TCommonMessage | undefined = await this.#store.get(\n        parentMessageId,\n      )\n      if (msg) {\n        messages.unshift(msg)\n        cnt++\n      }\n      parentMessageId = msg?.parentMessageId\n    }\n    if (this.#debug) {\n      this.#log('[ConversationStore getMessages end]', messages)\n    }\n    return messages\n  }\n  /**\n   * clear the store\n   */\n  async clearAll() {\n    await this.#store.clear()\n  }\n  getStoreSize(): number {\n    return this.#lru.size\n  }\n}\n","import { AxiosRequestConfig } from 'axios'\nimport { TiktokenEmbedding } from '@dqbd/tiktoken'\n/**\n * ChatCompletion 回答\n */\nexport interface IChatCompletion {\n  // {\n  //   \"id\": \"chatcmpl-6psB2OWQOgHwCWzTIsQMnhB3fst1J\",\n  //   \"object\": \"chat.completion\",\n  //   \"created\": 1677821004,\n  //   \"model\": \"gpt-3.5-turbo-0301\",\n  //   \"usage\": {\n  //     \"prompt_tokens\": 47,\n  //     \"completion_tokens\": 391,\n  //     \"total_tokens\": 438\n  //   },\n  //   \"choices\": [\n  //     {\n  //       \"message\": {\n  //         \"role\": \"assistant\",\n  //         \"content\": \"回答\"\n  //       },\n  //       \"finish_reason\": \"stop\",\n  //       \"index\": 0\n  //     }\n  //   ]\n  // }\n\n  id: string\n  object: string\n  created: number\n  model: string\n  usage: {\n    prompt_tokens: number\n    completion_tokens: number\n    total_tokens: number\n  }\n  choices: {\n    // content 即为答案\n    message: { role: string; content: string }\n    finish_reason: string\n    index: number\n  }[]\n}\n\nexport interface IChatCompletionErrReponseData {\n  message?: string\n  type?: string\n}\n\nexport interface IChatCompletionStreamOnEndData {\n  success: boolean\n  data: IChatGPTResponse | IChatCompletionErrReponseData\n  status: number\n}\n\nexport type TChatCompletionStreamOnEnd = (\n  endData: IChatCompletionStreamOnEndData,\n) => void\n\n/**\n * response message\n */\nexport interface IChatGPTResponse {\n  id: string\n  text: string\n  created: number\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\n/**\n * user message\n */\nexport interface IChatGPTUserMessage {\n  id: string\n  text: string\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\n/**\n * system situation message\n */\nexport interface IChatGPTSystemMessage {\n  id: string\n  text: string\n  role: ERole\n  parentMessageId?: string\n  tokens?: number\n}\nexport interface IChatGPTHTTPDataMessage {\n  role: ERole\n  content: string\n}\n\nexport enum ERole {\n  /**\n   * conversation situation\n   */\n  system = 'system',\n  /**\n   * role is user\n   */\n  user = 'user',\n  /**\n   * role is chatgpt\n   */\n  assistant = 'assistant',\n}\n\nexport interface IConversationStoreParams {\n  maxKeys?: number\n  maxFindDepth?: number\n  debug: boolean\n  log: TLog\n}\n\nexport interface IChatGPTParams {\n  /**\n   * apiKey, you can get it in https://platform.openai.com/account/api-keys,You can apply for up to 5 at most.\n   */\n  apiKey: string\n  /**\n   * model，default is 'gpt-3.5-turbo'\n   */\n  model?: string\n  /**\n   * print logs\n   */\n  debug?: boolean\n  /**\n   * axios configs\n   */\n  requestConfig?: AxiosRequestConfig\n  /**\n   * configs for store\n   */\n  storeConfig?: {\n    /**\n     * lru max keys, default `100000`\n     */\n    maxKeys?: number\n    /**\n     * Recursively search for historical messages, default `20` messages will be sent to the ChatGPT server\n     */\n    maxFindDepth?: number\n  }\n  tokenizerConfig?: ITokensParams\n  /**\n   * the maximum number of tokens when initiating a request, including prompts and completion. The default value is 4096.\n   */\n  maxTokens?: number\n  /**\n   * The maximum number of tokens for a single message. It is used to prevent from sending too many tokens to the ChatGPT server.\n   * If this number is exceeded, the message will be deleted and not passed on as a prompt to the chatGPT server. The default value is `1000`.\n   * - notice: **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**.\n   */\n  limitTokensInAMessage?: number\n  /**\n   * same reason as `limitTokensInAMessage`, **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**, default value is `false`\n   * - `true`: will ignore ChatGPT server message in the next sendMessage, and will only refer to `limitTokensInAMessage` in history messages\n   * - `false`: will only refer to `limitTokensInAMessage` in history messages\n   */\n  ignoreServerMessagesInPrompt?: boolean\n\n  log?: TLog\n}\n\n/**\n * Tokenizer params\n */\nexport interface ITokensParams {\n  /**\n   * \"gpt2\" | \"r50k_base\" | \"p50k_base\" | \"p50k_edit\" | \"cl100k_base\", default 'cl100k_base'\n   */\n  encoding?: TiktokenEmbedding\n\n  /**\n   * replace regexp\n   */\n  replaceReg?: RegExp\n  /**\n   * replace function\n   */\n  replaceCallback?: (...args: any[]) => string\n}\n\nexport type TCommonMessage =\n  | IChatGPTResponse\n  | IChatGPTUserMessage\n  | IChatGPTSystemMessage\n\n/**\n * Pass in your own logger\n */\nexport type TLog = (msg: string, ...args: any[]) => void\n\nexport interface ISendMessagesOpts {\n  text?: string\n  systemPrompt?: string\n  parentMessageId?: string\n  onProgress?: (t: string) => void\n  onEnd?: (d: IChatCompletionStreamOnEndData) => void\n  initialMessages?: TCommonMessage[]\n}\n","import { get_encoding, Tiktoken } from '@dqbd/tiktoken'\nimport { ITokensParams, TCommonMessage } from './types'\n\nexport default class Tokenizer {\n  #tokenizer: Tiktoken\n  #replaceReg: RegExp\n  #replaceCallback: (...args: any[]) => string\n  constructor(opts: ITokensParams) {\n    const {\n      encoding = 'cl100k_base',\n      replaceReg = /<\\|endoftext\\|>/g,\n      replaceCallback = (...args: any[]) => '',\n    } = opts\n    this.#tokenizer = get_encoding(encoding)\n    this.#replaceReg = replaceReg\n    this.#replaceCallback = replaceCallback\n  }\n  #encode(text: string): Uint32Array {\n    return this.#tokenizer.encode(text)\n  }\n  /**\n   * get the text tokens count\n   * @param text\n   * @returns\n   */\n  getTokenCnt(msg: TCommonMessage | string){\n    if(typeof msg === 'object' && msg.tokens) return msg.tokens\n    msg = typeof msg === 'object' ? msg.text : msg\n    const text = msg.replace(this.#replaceReg, this.#replaceCallback)\n    return this.#encode(text).length\n  }\n}\n\n// const token = new Tokenizer({\n//   replaceReg: /hello|world/g,\n//   replaceCallback(c: string) {\n//     return 'ok'\n//   },\n// })\n// console.log(token.getTokenCnt('hello world'))\n","import axios from 'axios'\nimport { RawAxiosRequestConfig } from 'axios'\nimport { TLog } from '../types'\n\ninterface IRequestOpts {\n  debug: boolean\n  log: TLog\n}\n\n// export async function get(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n//   const ins = axios.create({\n//     method: 'GET',\n//   })\n//   if (opts.debug) {\n//     ins.interceptors.request.use((config) => {\n//       log('axios config', config)\n//       return config\n//     })\n//   }\n//   return (await ins({ ...config })).data\n// }\nexport async function post(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n  const { debug, log } = opts\n  const ins = axios.create({\n    method: 'POST',\n    validateStatus(status) {\n      return true\n    },\n  })\n  if (debug) {\n    ins.interceptors.request.use((config) => {\n      log('axios config', {\n        headers: config.headers,\n        data: config.data,\n      })\n      return config\n    })\n  }\n  const response = await ins({ ...config })\n  return response\n}\n\n// if (response.status >= 200 && response.status < 300) {\n//   console.log('[response]', response.data)\n// } else {\n//   console.log('[response]', {\n//     data: {\n//       message: response.data.error.message,\n//       type: response.data.error.type,\n//       code: response.data.error.code,\n//     },\n//     status: response.status,\n//   })\n// }\n","/**\n * docs https://platform.openai.com/docs/api-reference/chat\n */\nconst urls = {\n  listModels: 'https://api.openai.com/v1/models', // get\n  createCompletion: 'https://api.openai.com/v1/completions', // post\n  createChatCompletion: 'https://api.openai.com/v1/chat/completions' // post\n}\n\nexport default urls\n","import { v4 as uuid } from 'uuid'\nimport { stdin, stdout } from 'process'\nimport { createInterface } from 'readline'\nexport * from './log'\n\nexport function getReadLine() {\n  const rl = createInterface({ input: stdin, output: stdout })\n  const iter = rl[Symbol.asyncIterator]()\n  return async () => (await iter.next()).value\n}\n\n/**\n * generate unique id by uuidV4\n */\nexport function genId() {\n  return uuid()\n}\n\nexport function isString(target: any) {\n  return Object.prototype.toString.call(target) === '[object String]'\n}\n\nexport function isArray(target: any) {\n  return Object.prototype.toString.call(target) === '[object Array]'\n}\nexport function isObject(target: any) {\n  return Object.prototype.toString.call(target) === '[object Object]'\n}\n","export function log(...args: any[]) {\n  console.log('----------------------------------')\n  console.log(...args)\n}\n","import { AxiosRequestConfig } from 'axios'\n\nimport ConversationStore from './ConversationStore'\nimport Tokenizer from './Tokenizer'\nimport {\n  IChatCompletion,\n  IChatGPTResponse,\n  IChatGPTUserMessage,\n  IChatGPTSystemMessage,\n  ERole,\n  IChatGPTHTTPDataMessage,\n  IChatGPTParams,\n  IChatCompletionStreamOnEndData,\n  IChatCompletionErrReponseData,\n  TLog,\n  TCommonMessage,\n  ISendMessagesOpts,\n} from './types'\nimport { post } from './utils/request'\nimport URLS from './utils/urls'\nimport { genId, log as defaultLog, isString, isArray } from './utils'\n\n// https://platform.openai.com/docs/api-reference/chat\n// curl https://api.openai.com/v1/chat/completions \\\n//   -H 'Content-Type: application/json' \\\n//   -H 'Authorization: Bearer YOUR_API_KEY' \\\n//   -d '{\n//   \"model\": \"gpt-3.5-turbo\",\n//   \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n// }'\n\nfunction genDefaultSystemMessage(): IChatGPTHTTPDataMessage {\n  const currentDate = new Date().toISOString().split('T')[0]\n  return {\n    role: ERole.system,\n    content: `You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\\nKnowledge cutoff: 2021-09-01\\nCurrent date: ${currentDate}`,\n  }\n}\n\n// role https://platform.openai.com/docs/guides/chat/introduction\nexport class ChatGPT {\n  #apiKey = ''\n  #model = ''\n  #urls = URLS\n  #debug = false\n  #requestConfig: AxiosRequestConfig\n  #store: ConversationStore\n  #tokenizer: Tokenizer\n  #maxTokens: number\n  #limitTokensInAMessage: number\n  #ignoreServerMessagesInPrompt: boolean\n  #log: TLog\n  constructor(opts: IChatGPTParams) {\n    const {\n      apiKey,\n      model = 'gpt-3.5-turbo',\n      debug = false,\n      requestConfig = {},\n      storeConfig = {},\n      tokenizerConfig = {},\n      maxTokens = 4096,\n      limitTokensInAMessage = 1000,\n      ignoreServerMessagesInPrompt = false,\n      log = defaultLog,\n    } = opts\n\n    this.#apiKey = apiKey\n    this.#model = model\n    this.#debug = debug\n    this.#requestConfig = requestConfig\n    this.#tokenizer = new Tokenizer(tokenizerConfig)\n    this.#maxTokens = maxTokens\n    this.#limitTokensInAMessage = limitTokensInAMessage\n    this.#ignoreServerMessagesInPrompt = ignoreServerMessagesInPrompt\n    this.#log = log\n\n    this.#store = new ConversationStore({\n      ...storeConfig,\n      debug: this.#debug,\n      log: this.#log,\n    })\n  }\n\n  /**\n   * get related messages\n   * @param parentMessageId\n   */\n  async getMessages(opts: {\n    id: string\n    maxDepth?: number\n  }): Promise<TCommonMessage[]> {\n    const messages = await this.#store.getMessages(opts)\n    return messages\n  }\n\n  /**\n   * add messages to store\n   * @param messages\n   * @returns\n   */\n  async addMessages(messages: TCommonMessage[]) {\n    return await this.#store.set(messages)\n  }\n\n  /**\n   * send message to ChatGPT server\n   * @param opts.text new message\n   * @param opts.systemPrompt prompt message\n   * @param opts.parentMessageId\n   */\n  sendMessage(opts: ISendMessagesOpts | string | TCommonMessage[]) {\n    return new Promise<IChatCompletionStreamOnEndData | null>(\n      async (resolve, reject) => {\n        if (isString(opts)) {\n          opts = { text: opts as string }\n        } else if (isArray(opts)) {\n          opts = { initialMessages: opts as TCommonMessage[] }\n        } else {\n          // 使用对象传入，必须要设置 text\n          if (\n            !(opts as ISendMessagesOpts).text &&\n            !(opts as ISendMessagesOpts).initialMessages\n          ) {\n            return reject(\n              'You are passing in an object and it is required to set the text or initialMessages attribute.',\n            )\n          }\n        }\n        let {\n          text = '',\n          systemPrompt = undefined,\n          parentMessageId = undefined,\n          onProgress = false,\n          onEnd = () => {},\n          initialMessages = undefined,\n        } = opts as ISendMessagesOpts\n        // 是否需要把数据存储到 store 中\n        const shouldAddToStore = !initialMessages\n        if (systemPrompt) {\n          if (parentMessageId)\n            await this.#store.clear1Conversation(parentMessageId)\n          parentMessageId = undefined\n        }\n        const userMessage: IChatGPTUserMessage = {\n          id: genId(),\n          text,\n          role: ERole.user,\n          parentMessageId,\n          tokens: this.#tokenizer.getTokenCnt(text),\n        }\n        let messages: IChatGPTHTTPDataMessage[] = []\n        if (shouldAddToStore) {\n          messages = await this.#makeConversations(userMessage, systemPrompt)\n        } else {\n          messages = (initialMessages as TCommonMessage[]).map((msg) => ({\n            role: msg.role,\n            content: msg.text,\n          }))\n        }\n        if (this.#debug) {\n          this.#log('history messages', messages)\n        }\n        if (onProgress) {\n          const responseMessage: IChatGPTResponse = {\n            id: genId(),\n            text: '',\n            created: Math.floor(Date.now() / 1000),\n            role: ERole.assistant,\n            parentMessageId: shouldAddToStore\n              ? userMessage.id\n              : (initialMessages as TCommonMessage[])[\n                  (initialMessages as TCommonMessage[]).length - 1\n                ].id,\n            tokens: 0,\n          }\n          const innerOnEnd = async () => {\n            if (shouldAddToStore) {\n              const msgsToBeStored = [userMessage, responseMessage]\n              if (systemPrompt) {\n                const systemMessage: IChatGPTSystemMessage = {\n                  id: genId(),\n                  text: systemPrompt,\n                  role: ERole.system,\n                  tokens: this.#tokenizer.getTokenCnt(systemPrompt),\n                }\n                userMessage.parentMessageId = systemMessage.id\n                msgsToBeStored.unshift(systemMessage)\n              }\n              await this.#store.set(msgsToBeStored)\n            }\n            resolve(null)\n          }\n          await this.#streamChat(\n            messages,\n            onProgress,\n            responseMessage,\n            innerOnEnd,\n            onEnd,\n          )\n        } else {\n          const chatResponse = await this.#chat(messages)\n          if (!chatResponse.success) {\n            return resolve({\n              ...chatResponse,\n              data: chatResponse.data as IChatCompletionErrReponseData,\n            })\n          }\n          const res = chatResponse.data as IChatCompletion\n          const responseMessage: IChatGPTResponse = {\n            id: genId(),\n            text: res?.choices[0]?.message?.content,\n            created: res.created,\n            role: ERole.assistant,\n            parentMessageId: shouldAddToStore\n              ? userMessage.id\n              : (initialMessages as TCommonMessage[])[\n                  (initialMessages as TCommonMessage[]).length - 1\n                ].id,\n            tokens: res?.usage?.completion_tokens,\n          }\n          if (shouldAddToStore) {\n            const msgsToBeStored = [userMessage, responseMessage]\n            if (systemPrompt) {\n              const systemMessage: IChatGPTSystemMessage = {\n                id: genId(),\n                text: systemPrompt,\n                role: ERole.system,\n                tokens: this.#tokenizer.getTokenCnt(systemPrompt),\n              }\n              userMessage.parentMessageId = systemMessage.id\n              msgsToBeStored.unshift(systemMessage)\n            }\n            await this.#store.set(msgsToBeStored)\n          }\n          resolve({\n            success: true,\n            data: responseMessage,\n            status: chatResponse.status,\n          })\n        }\n      },\n    )\n  }\n\n  async #streamChat(\n    messages: { content: string; role: ERole }[],\n    onProgress: boolean | ((t: string) => void),\n    responseMessagge: IChatGPTResponse,\n    innerOnEnd: () => void,\n    onEnd?: (d: IChatCompletionStreamOnEndData) => void,\n  ) {\n    const axiosResponse = await post(\n      {\n        url: this.#urls.createChatCompletion,\n        ...this.#requestConfig,\n        headers: {\n          Authorization: this.#genAuthorization(),\n          'Content-Type': 'application/json',\n          ...{ ...(this.#requestConfig.headers || {}) },\n        },\n        data: {\n          stream: true,\n          model: this.#model,\n          messages,\n          ...{ ...(this.#requestConfig.data || {}) },\n        },\n        responseType: 'stream',\n      },\n      {\n        debug: this.#debug,\n        log: this.#log,\n      },\n    )\n    const stream = axiosResponse.data\n    const status = axiosResponse.status\n    if (this.#validateAxiosResponse(status)) {\n      stream.on('data', (buf: any) => {\n        const dataArr = buf.toString().split('\\n')\n        let onDataPieceText = ''\n        for (const dataStr of dataArr) {\n          // split 之后的空行，或者结束通知\n          if (dataStr.indexOf('data: ') !== 0 || dataStr === 'data: [DONE]')\n            continue\n          const parsedData = JSON.parse(dataStr.slice(6)) // [data: ]\n          const pieceText = parsedData.choices[0].delta.content || ''\n          onDataPieceText += pieceText\n        }\n        if (typeof onProgress === 'function') {\n          onProgress(onDataPieceText)\n        }\n        responseMessagge.text += onDataPieceText\n      })\n      stream.on('end', async () => {\n        responseMessagge.tokens = this.#tokenizer.getTokenCnt(\n          responseMessagge.text,\n        )\n        await innerOnEnd()\n        onEnd &&\n          onEnd({\n            success: true,\n            data: responseMessagge,\n            status: axiosResponse.status,\n          })\n      })\n    } else {\n      let data: any = stream.on('data', (buf: any) => {\n        data = JSON.parse(buf.toString())\n        // that is stream\n        // error: {\n        //     message: 'Your access was terminated due to violation of our policies, please check your email for more information. If you believe this is in error and would like to appeal, please contact support@openai.com.',\n        //     type: 'access_terminated',\n        //     param: null,\n        //     code: null\n        //   }\n        // }\n      })\n      stream.on('end', () => {\n        onEnd &&\n          onEnd({\n            success: false,\n            data: {\n              message: data?.error?.message,\n              type: data?.error?.type,\n            },\n            status,\n          })\n      })\n    }\n  }\n\n  async #chat(messages: { content: string; role: ERole }[]) {\n    const axiosResponse = await post(\n      {\n        url: this.#urls.createChatCompletion,\n        ...this.#requestConfig,\n        headers: {\n          Authorization: this.#genAuthorization(),\n          'Content-Type': 'application/json',\n          ...{ ...(this.#requestConfig.headers || {}) },\n        },\n        data: {\n          model: this.#model,\n          messages,\n          ...{ ...(this.#requestConfig.data || {}) },\n        },\n      },\n      {\n        debug: this.#debug,\n        log: this.#log,\n      },\n    )\n    // this.#logger('[#chat]', axiosResponse.status)\n    const data = axiosResponse.data\n    const status = axiosResponse.status\n    if (this.#validateAxiosResponse(status)) {\n      return {\n        success: true,\n        data: data as IChatCompletion,\n        status,\n      }\n    } else {\n      return {\n        success: false,\n        data: {\n          message: data?.error?.message,\n          type: data?.error?.type,\n        },\n        status,\n      }\n    }\n  }\n\n  #validateAxiosResponse(status: number) {\n    return status >= 200 && status < 300\n  }\n\n  /**\n   * make conversations for http request data.messages\n   */\n  async #makeConversations(userMessage: IChatGPTUserMessage, prompt?: string) {\n    let messages: IChatGPTHTTPDataMessage[] = []\n    let usedTokens = this.#tokenizer.getTokenCnt(userMessage.text)\n    if (prompt) {\n      messages.push({\n        role: ERole.system,\n        content: prompt,\n      })\n    } else {\n      messages = await this.#store.findMessages({\n        id: userMessage.parentMessageId,\n        tokenizer: this.#tokenizer,\n        limit: this.#limitTokensInAMessage,\n        availableTokens: this.#maxTokens - usedTokens,\n        ignore: this.#ignoreServerMessagesInPrompt,\n      })\n    }\n    /**\n     * if there are no default system massage, add one\n     */\n    if (!messages.length || messages[0].role !== ERole.system) {\n      messages.unshift(genDefaultSystemMessage())\n    }\n    messages.push({\n      role: ERole.user,\n      content: userMessage.text,\n    })\n    return messages\n  }\n\n  async clear1Conversation(parentMessageId?: string) {\n    return await this.#store.clear1Conversation(parentMessageId)\n  }\n\n  /**\n   * generate HTTP Authorization\n   * @returns\n   */\n  #genAuthorization() {\n    return `Bearer ${this.#apiKey}`\n  }\n\n  getStoreSize() {\n    return this.#store.getStoreSize()\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAA,kBAAiB;AACjB,uBAAqB;;;AC+Fd,IAAK,QAAL,kBAAKA,WAAL;AAIL,EAAAA,OAAA,YAAS;AAIT,EAAAA,OAAA,UAAO;AAIP,EAAAA,OAAA,eAAY;AAZF,SAAAA;AAAA,GAAA;;;ADhGZ;AAeA,IAAqB,oBAArB,MAAuC;AAAA,EASrC,YAAY,QAAkC;AAR9C;AACA;AAIA;AAAA;AAAA;AAAA;AACA;AACA;AAEE,UAAM,EAAE,UAAU,KAAQ,eAAe,IAAI,OAAO,KAAAC,KAAI,IAAI;AAC5D,uBAAK,MAAO,IAAI,iBAAAC,QAAiC;AAAA,MAC/C,KAAK;AAAA,IACP,CAAC;AACD,uBAAK,QAAS,IAAI,YAAAC,QAA0B;AAAA,MAC1C,OAAO,mBAAK;AAAA,IACd,CAAC;AACD,uBAAK,eAAgB;AACrB,uBAAK,QAAS;AACd,uBAAK,MAAOF;AAEZ,QAAI,mBAAK;AAAQ,yBAAK,MAAL,WAAU,4BAA4B;AAAA,EACzD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAAiD;AACzD,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,MAAwB;AAChC,eAAW,OAAO,MAAM;AACtB,YAAM,mBAAK,QAAO,IAAI,IAAI,IAAI,GAAG;AAAA,IACnC;AACA,QAAI,mBAAK;AAAQ,yBAAK,MAAL,WAAU,YAAY,mBAAK,MAAK;AAAA,EACnD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAA8B;AACtC,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,OAAO,IAA8B;AACzC,WAAO,MAAM,mBAAK,QAAO,OAAO,EAAE;AAAA,EACpC;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAmB,IAAa;AACpC,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD;AACA,YAAM,MAAkC,MAAM,KAAK,IAAI,eAAe;AACtE,UAAI,KAAK;AACP,cAAM,KAAK,OAAO,IAAI,EAAE;AAAA,MAC1B;AACA,wBAAkB,2BAAK;AAAA,IACzB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,MAMhB;AACD,QAAI;AAAA,MACF,KAAK;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA,SAAS;AAAA,IACX,IAAI;AACJ,QAAI,mBAAK,SAAQ;AACf,yBAAK,MAAL,WAAU,0CAA0C;AAAA,IACtD;AACA,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,UAAM,WAAsC,CAAC;AAC7C,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD,YAAM,MAAkC,MAAM,mBAAK,QAAO;AAAA,QACxD;AAAA,MACF;AACA,UAAI,OAAO,EAAE,UAAU,IAAI,uCAA2B;AACpD,YAAI,YAAY,IAAI,UAAU,UAAU,YAAY,IAAI,IAAI;AAC5D,YAAI,aAAa,OAAO;AACtB,cAAI,kBAAkB;AAAW;AACjC,mBAAS,QAAQ;AAAA,YACf,MAAM,IAAI;AAAA,YACV,SAAS,IAAI;AAAA,UACf,CAAC;AACD;AACA,6BAAmB;AAAA,QACrB;AAAA,MACF;AACA,wBAAkB,2BAAK;AAAA,IACzB;AACA,QAAI,mBAAK,SAAQ;AACf,yBAAK,MAAL,WAAU,mBAAmB;AAC7B,yBAAK,MAAL,WAAU,wCAAwC;AAAA,IACpD;AACA,WAAO;AAAA,EACT;AAAA,EAEA,MAAM,YAAY,MAAyC;AACzD,QAAI,EAAE,KAAK,QAAW,WAAW,GAAG,IAAI;AACxC,QAAI,mBAAK,SAAQ;AACf,yBAAK,MAAL,WAAU,yCAAyC,EAAE,IAAI,SAAS;AAAA,IACpE;AACA,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,UAAM,WAA6B,CAAC;AACpC,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD,YAAM,MAAkC,MAAM,mBAAK,QAAO;AAAA,QACxD;AAAA,MACF;AACA,UAAI,KAAK;AACP,iBAAS,QAAQ,GAAG;AACpB;AAAA,MACF;AACA,wBAAkB,2BAAK;AAAA,IACzB;AACA,QAAI,mBAAK,SAAQ;AACf,yBAAK,MAAL,WAAU,uCAAuC;AAAA,IACnD;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAIA,MAAM,WAAW;AACf,UAAM,mBAAK,QAAO,MAAM;AAAA,EAC1B;AAAA,EACA,eAAuB;AACrB,WAAO,mBAAK,MAAK;AAAA,EACnB;AACF;AA1JE;AACA;AAIA;AACA;AACA;;;AEvBF,sBAAuC;AAAvC;AAGA,IAAqB,YAArB,MAA+B;AAAA,EAI7B,YAAY,MAAqB;AAUjC;AAbA;AACA;AACA;AAEE,UAAM;AAAA,MACJ,WAAW;AAAA,MACX,aAAa;AAAA,MACb,kBAAkB,IAAI,SAAgB;AAAA,IACxC,IAAI;AACJ,uBAAK,gBAAa,8BAAa,QAAQ;AACvC,uBAAK,aAAc;AACnB,uBAAK,kBAAmB;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,YAAY,KAA6B;AACvC,QAAG,OAAO,QAAQ,YAAY,IAAI;AAAQ,aAAO,IAAI;AACrD,UAAM,OAAO,QAAQ,WAAW,IAAI,OAAO;AAC3C,UAAM,OAAO,IAAI,QAAQ,mBAAK,cAAa,mBAAK,iBAAgB;AAChE,WAAO,sBAAK,oBAAL,WAAa,MAAM;AAAA,EAC5B;AACF;AA3BE;AACA;AACA;AAWA;AAAA,YAAO,SAAC,MAA2B;AACjC,SAAO,mBAAK,YAAW,OAAO,IAAI;AACpC;;;ACnBF,mBAAkB;AAqBlB,eAAsB,KAAK,QAA+B,MAAoB;AAC5E,QAAM,EAAE,OAAO,KAAAG,KAAI,IAAI;AACvB,QAAM,MAAM,aAAAC,QAAM,OAAO;AAAA,IACvB,QAAQ;AAAA,IACR,eAAe,QAAQ;AACrB,aAAO;AAAA,IACT;AAAA,EACF,CAAC;AACD,MAAI,OAAO;AACT,QAAI,aAAa,QAAQ,IAAI,CAACC,YAAW;AACvC,MAAAF,KAAI,gBAAgB;AAAA,QAClB,SAASE,QAAO;AAAA,QAChB,MAAMA,QAAO;AAAA,MACf,CAAC;AACD,aAAOA;AAAA,IACT,CAAC;AAAA,EACH;AACA,QAAM,WAAW,MAAM,IAAI,EAAE,GAAG,OAAO,CAAC;AACxC,SAAO;AACT;;;ACrCA,IAAM,OAAO;AAAA,EACX,YAAY;AAAA;AAAA,EACZ,kBAAkB;AAAA;AAAA,EAClB,sBAAsB;AAAA;AACxB;AAEA,IAAO,eAAQ;;;ACTf,kBAA2B;;;ACApB,SAAS,OAAO,MAAa;AAClC,UAAQ,IAAI,oCAAoC;AAChD,UAAQ,IAAI,GAAG,IAAI;AACrB;;;ADWO,SAAS,QAAQ;AACtB,aAAO,YAAAC,IAAK;AACd;AAEO,SAAS,SAAS,QAAa;AACpC,SAAO,OAAO,UAAU,SAAS,KAAK,MAAM,MAAM;AACpD;AAEO,SAAS,QAAQ,QAAa;AACnC,SAAO,OAAO,UAAU,SAAS,KAAK,MAAM,MAAM;AACpD;;;AEOA,SAAS,0BAAmD;AAC1D,QAAM,eAAc,oBAAI,KAAK,GAAE,YAAY,EAAE,MAAM,GAAG,EAAE,CAAC;AACzD,SAAO;AAAA,IACL;AAAA,IACA,SAAS;AAAA;AAAA,gBAA4I;AAAA,EACvJ;AACF;AArCA,4BAAAC,SAAA,gBAAAC,SAAAC,aAAA,mEAAAC,OAAA;AAwCO,IAAM,UAAN,MAAc;AAAA,EAYnB,YAAY,MAAsB;AAgMlC,uBAAM;AAsFN,uBAAM;AA0CN;AAOA;AAAA;AAAA;AAAA,uBAAM;AAsCN;AAAA;AAAA;AAAA;AAAA;AAxXA,gCAAU;AACV,+BAAS;AACT,8BAAQ;AACR,uBAAAH,SAAS;AACT;AACA,uBAAAC,SAAA;AACA,uBAAAC,aAAA;AACA;AACA;AACA;AACA,uBAAAC,OAAA;AAEE,UAAM;AAAA,MACJ;AAAA,MACA,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,gBAAgB,CAAC;AAAA,MACjB,cAAc,CAAC;AAAA,MACf,kBAAkB,CAAC;AAAA,MACnB,YAAY;AAAA,MACZ,wBAAwB;AAAA,MACxB,+BAA+B;AAAA,MAC/B,KAAAC,OAAM;AAAA,IACR,IAAI;AAEJ,uBAAK,SAAU;AACf,uBAAK,QAAS;AACd,uBAAKJ,SAAS;AACd,uBAAK,gBAAiB;AACtB,uBAAKE,aAAa,IAAI,UAAU,eAAe;AAC/C,uBAAK,YAAa;AAClB,uBAAK,wBAAyB;AAC9B,uBAAK,+BAAgC;AACrC,uBAAKC,OAAOC;AAEZ,uBAAKH,SAAS,IAAI,kBAAkB;AAAA,MAClC,GAAG;AAAA,MACH,OAAO,mBAAKD;AAAA,MACZ,KAAK,mBAAKG;AAAA,IACZ,CAAC;AAAA,EACH;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,YAAY,MAGY;AAC5B,UAAM,WAAW,MAAM,mBAAKF,SAAO,YAAY,IAAI;AACnD,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,YAAY,UAA4B;AAC5C,WAAO,MAAM,mBAAKA,SAAO,IAAI,QAAQ;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,YAAY,MAAqD;AAC/D,WAAO,IAAI;AAAA,MACT,OAAO,SAAS,WAAW;AAhHjC;AAiHQ,YAAI,SAAS,IAAI,GAAG;AAClB,iBAAO,EAAE,MAAM,KAAe;AAAA,QAChC,WAAW,QAAQ,IAAI,GAAG;AACxB,iBAAO,EAAE,iBAAiB,KAAyB;AAAA,QACrD,OAAO;AAEL,cACE,CAAE,KAA2B,QAC7B,CAAE,KAA2B,iBAC7B;AACA,mBAAO;AAAA,cACL;AAAA,YACF;AAAA,UACF;AAAA,QACF;AACA,YAAI;AAAA,UACF,OAAO;AAAA,UACP,eAAe;AAAA,UACf,kBAAkB;AAAA,UAClB,aAAa;AAAA,UACb,QAAQ,MAAM;AAAA,UAAC;AAAA,UACf,kBAAkB;AAAA,QACpB,IAAI;AAEJ,cAAM,mBAAmB,CAAC;AAC1B,YAAI,cAAc;AAChB,cAAI;AACF,kBAAM,mBAAKA,SAAO,mBAAmB,eAAe;AACtD,4BAAkB;AAAA,QACpB;AACA,cAAM,cAAmC;AAAA,UACvC,IAAI,MAAM;AAAA,UACV;AAAA,UACA;AAAA,UACA;AAAA,UACA,QAAQ,mBAAKC,aAAW,YAAY,IAAI;AAAA,QAC1C;AACA,YAAI,WAAsC,CAAC;AAC3C,YAAI,kBAAkB;AACpB,qBAAW,MAAM,sBAAK,0CAAL,WAAwB,aAAa;AAAA,QACxD,OAAO;AACL,qBAAY,gBAAqC,IAAI,CAAC,SAAS;AAAA,YAC7D,MAAM,IAAI;AAAA,YACV,SAAS,IAAI;AAAA,UACf,EAAE;AAAA,QACJ;AACA,YAAI,mBAAKF,UAAQ;AACf,6BAAKG,OAAL,WAAU,oBAAoB;AAAA,QAChC;AACA,YAAI,YAAY;AACd,gBAAM,kBAAoC;AAAA,YACxC,IAAI,MAAM;AAAA,YACV,MAAM;AAAA,YACN,SAAS,KAAK,MAAM,KAAK,IAAI,IAAI,GAAI;AAAA,YACrC;AAAA,YACA,iBAAiB,mBACb,YAAY,KACX,gBACE,gBAAqC,SAAS,CACjD,EAAE;AAAA,YACN,QAAQ;AAAA,UACV;AACA,gBAAM,aAAa,YAAY;AAC7B,gBAAI,kBAAkB;AACpB,oBAAM,iBAAiB,CAAC,aAAa,eAAe;AACpD,kBAAI,cAAc;AAChB,sBAAM,gBAAuC;AAAA,kBAC3C,IAAI,MAAM;AAAA,kBACV,MAAM;AAAA,kBACN;AAAA,kBACA,QAAQ,mBAAKD,aAAW,YAAY,YAAY;AAAA,gBAClD;AACA,4BAAY,kBAAkB,cAAc;AAC5C,+BAAe,QAAQ,aAAa;AAAA,cACtC;AACA,oBAAM,mBAAKD,SAAO,IAAI,cAAc;AAAA,YACtC;AACA,oBAAQ,IAAI;AAAA,UACd;AACA,gBAAM,sBAAK,4BAAL,WACJ,UACA,YACA,iBACA,YACA;AAAA,QAEJ,OAAO;AACL,gBAAM,eAAe,MAAM,sBAAK,gBAAL,WAAW;AACtC,cAAI,CAAC,aAAa,SAAS;AACzB,mBAAO,QAAQ;AAAA,cACb,GAAG;AAAA,cACH,MAAM,aAAa;AAAA,YACrB,CAAC;AAAA,UACH;AACA,gBAAM,MAAM,aAAa;AACzB,gBAAM,kBAAoC;AAAA,YACxC,IAAI,MAAM;AAAA,YACV,OAAM,sCAAK,QAAQ,OAAb,mBAAiB,YAAjB,mBAA0B;AAAA,YAChC,SAAS,IAAI;AAAA,YACb;AAAA,YACA,iBAAiB,mBACb,YAAY,KACX,gBACE,gBAAqC,SAAS,CACjD,EAAE;AAAA,YACN,SAAQ,gCAAK,UAAL,mBAAY;AAAA,UACtB;AACA,cAAI,kBAAkB;AACpB,kBAAM,iBAAiB,CAAC,aAAa,eAAe;AACpD,gBAAI,cAAc;AAChB,oBAAM,gBAAuC;AAAA,gBAC3C,IAAI,MAAM;AAAA,gBACV,MAAM;AAAA,gBACN;AAAA,gBACA,QAAQ,mBAAKC,aAAW,YAAY,YAAY;AAAA,cAClD;AACA,0BAAY,kBAAkB,cAAc;AAC5C,6BAAe,QAAQ,aAAa;AAAA,YACtC;AACA,kBAAM,mBAAKD,SAAO,IAAI,cAAc;AAAA,UACtC;AACA,kBAAQ;AAAA,YACN,SAAS;AAAA,YACT,MAAM;AAAA,YACN,QAAQ,aAAa;AAAA,UACvB,CAAC;AAAA,QACH;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAAA,EAuKA,MAAM,mBAAmB,iBAA0B;AACjD,WAAO,MAAM,mBAAKA,SAAO,mBAAmB,eAAe;AAAA,EAC7D;AAAA,EAUA,eAAe;AACb,WAAO,mBAAKA,SAAO,aAAa;AAAA,EAClC;AACF;AA/XE;AACA;AACA;AACAD,UAAA;AACA;AACAC,UAAA;AACAC,cAAA;AACA;AACA;AACA;AACAC,QAAA;AAiMM;AAAA,gBAAW,eACf,UACA,YACA,kBACA,YACA,OACA;AACA,QAAM,gBAAgB,MAAM;AAAA,IAC1B;AAAA,MACE,KAAK,mBAAK,OAAM;AAAA,MAChB,GAAG,mBAAK;AAAA,MACR,SAAS;AAAA,QACP,eAAe,sBAAK,wCAAL;AAAA,QACf,gBAAgB;AAAA,QAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,MAC9C;AAAA,MACA,MAAM;AAAA,QACJ,QAAQ;AAAA,QACR,OAAO,mBAAK;AAAA,QACZ;AAAA,QACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,MAC3C;AAAA,MACA,cAAc;AAAA,IAChB;AAAA,IACA;AAAA,MACE,OAAO,mBAAKH;AAAA,MACZ,KAAK,mBAAKG;AAAA,IACZ;AAAA,EACF;AACA,QAAM,SAAS,cAAc;AAC7B,QAAM,SAAS,cAAc;AAC7B,MAAI,sBAAK,kDAAL,WAA4B,SAAS;AACvC,WAAO,GAAG,QAAQ,CAAC,QAAa;AAC9B,YAAM,UAAU,IAAI,SAAS,EAAE,MAAM,IAAI;AACzC,UAAI,kBAAkB;AACtB,iBAAW,WAAW,SAAS;AAE7B,YAAI,QAAQ,QAAQ,QAAQ,MAAM,KAAK,YAAY;AACjD;AACF,cAAM,aAAa,KAAK,MAAM,QAAQ,MAAM,CAAC,CAAC;AAC9C,cAAM,YAAY,WAAW,QAAQ,CAAC,EAAE,MAAM,WAAW;AACzD,2BAAmB;AAAA,MACrB;AACA,UAAI,OAAO,eAAe,YAAY;AACpC,mBAAW,eAAe;AAAA,MAC5B;AACA,uBAAiB,QAAQ;AAAA,IAC3B,CAAC;AACD,WAAO,GAAG,OAAO,YAAY;AAC3B,uBAAiB,SAAS,mBAAKD,aAAW;AAAA,QACxC,iBAAiB;AAAA,MACnB;AACA,YAAM,WAAW;AACjB,eACE,MAAM;AAAA,QACJ,SAAS;AAAA,QACT,MAAM;AAAA,QACN,QAAQ,cAAc;AAAA,MACxB,CAAC;AAAA,IACL,CAAC;AAAA,EACH,OAAO;AACL,QAAI,OAAY,OAAO,GAAG,QAAQ,CAAC,QAAa;AAC9C,aAAO,KAAK,MAAM,IAAI,SAAS,CAAC;AAAA,IASlC,CAAC;AACD,WAAO,GAAG,OAAO,MAAM;AA5T7B;AA6TQ,eACE,MAAM;AAAA,QACJ,SAAS;AAAA,QACT,MAAM;AAAA,UACJ,UAAS,kCAAM,UAAN,mBAAa;AAAA,UACtB,OAAM,kCAAM,UAAN,mBAAa;AAAA,QACrB;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AACF;AAEM;AAAA,UAAK,eAAC,UAA8C;AA1U5D;AA2UI,QAAM,gBAAgB,MAAM;AAAA,IAC1B;AAAA,MACE,KAAK,mBAAK,OAAM;AAAA,MAChB,GAAG,mBAAK;AAAA,MACR,SAAS;AAAA,QACP,eAAe,sBAAK,wCAAL;AAAA,QACf,gBAAgB;AAAA,QAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,MAC9C;AAAA,MACA,MAAM;AAAA,QACJ,OAAO,mBAAK;AAAA,QACZ;AAAA,QACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,MAC3C;AAAA,IACF;AAAA,IACA;AAAA,MACE,OAAO,mBAAKF;AAAA,MACZ,KAAK,mBAAKG;AAAA,IACZ;AAAA,EACF;AAEA,QAAM,OAAO,cAAc;AAC3B,QAAM,SAAS,cAAc;AAC7B,MAAI,sBAAK,kDAAL,WAA4B,SAAS;AACvC,WAAO;AAAA,MACL,SAAS;AAAA,MACT;AAAA,MACA;AAAA,IACF;AAAA,EACF,OAAO;AACL,WAAO;AAAA,MACL,SAAS;AAAA,MACT,MAAM;AAAA,QACJ,UAAS,kCAAM,UAAN,mBAAa;AAAA,QACtB,OAAM,kCAAM,UAAN,mBAAa;AAAA,MACrB;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;AAEA;AAAA,2BAAsB,SAAC,QAAgB;AACrC,SAAO,UAAU,OAAO,SAAS;AACnC;AAKM;AAAA,uBAAkB,eAAC,aAAkC,QAAiB;AAC1E,MAAI,WAAsC,CAAC;AAC3C,MAAI,aAAa,mBAAKD,aAAW,YAAY,YAAY,IAAI;AAC7D,MAAI,QAAQ;AACV,aAAS,KAAK;AAAA,MACZ;AAAA,MACA,SAAS;AAAA,IACX,CAAC;AAAA,EACH,OAAO;AACL,eAAW,MAAM,mBAAKD,SAAO,aAAa;AAAA,MACxC,IAAI,YAAY;AAAA,MAChB,WAAW,mBAAKC;AAAA,MAChB,OAAO,mBAAK;AAAA,MACZ,iBAAiB,mBAAK,cAAa;AAAA,MACnC,QAAQ,mBAAK;AAAA,IACf,CAAC;AAAA,EACH;AAIA,MAAI,CAAC,SAAS,UAAU,SAAS,CAAC,EAAE,gCAAuB;AACzD,aAAS,QAAQ,wBAAwB,CAAC;AAAA,EAC5C;AACA,WAAS,KAAK;AAAA,IACZ;AAAA,IACA,SAAS,YAAY;AAAA,EACvB,CAAC;AACD,SAAO;AACT;AAUA;AAAA,sBAAiB,WAAG;AAClB,SAAO,UAAU,mBAAK;AACxB;","names":["ERole","log","LRUCache","Keyv","log","axios","config","uuid","_debug","_store","_tokenizer","_log","log"]}