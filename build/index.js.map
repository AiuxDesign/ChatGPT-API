{"version":3,"sources":["../src/index.ts","../src/ConversationStore.ts","../src/utils/index.ts","../src/utils/log.ts","../src/types.ts","../src/Tokenizer.ts","../src/utils/request.ts","../src/utils/urls.ts","../src/ChatGPT.ts"],"sourcesContent":["export * from './ChatGPT'\r\nexport * from './types'","import Keyv from 'keyv'\r\nimport LRUCache from 'lru-cache'\r\nimport Tokenizer from './Tokenizer'\r\nimport { log } from './utils'\r\n\r\nimport {\r\n  TChatGPTHTTPDataMessage,\r\n  IConversationStoreParams,\r\n  TCommonMessage,\r\n  ERole,\r\n} from './types'\r\n\r\n/**\r\n * conversation manage\r\n */\r\nexport default class ConversationStore {\r\n  #store: Keyv<TCommonMessage, any>\r\n  #lru: LRUCache<string, TCommonMessage>\r\n  /**\r\n   * in case some bad things happen\r\n   */\r\n  #maxFindDepth: number\r\n  #debug: boolean\r\n  constructor(params: IConversationStoreParams) {\r\n    const { maxKeys = 100000, maxFindDepth = 20, debug = false } = params\r\n    this.#lru = new LRUCache<string, TCommonMessage>({\r\n      max: maxKeys,\r\n    })\r\n    this.#store = new Keyv<TCommonMessage, any>({\r\n      store: this.#lru,\r\n    })\r\n    this.#maxFindDepth = maxFindDepth\r\n    this.#debug = debug\r\n\r\n    if (this.#debug) log('ConversationStore params', params)\r\n  }\r\n  /**\r\n   * get message by id\r\n   * @param id\r\n   * @returns\r\n   */\r\n  async get(id: string): Promise<TCommonMessage | undefined> {\r\n    return await this.#store.get(id)\r\n  }\r\n  /**\r\n   * set new message\r\n   * @param msg\r\n   * @returns\r\n   */\r\n  async set(msgs: TCommonMessage[]) {\r\n    for (const msg of msgs) {\r\n      await this.#store.set(msg.id, msg)\r\n    }\r\n    if (this.#debug) log('lru size', this.#lru.size)\r\n  }\r\n  /**\r\n   * check if the id exists in the store\r\n   * @param id\r\n   * @returns\r\n   */\r\n  async has(id: string): Promise<boolean> {\r\n    return await this.#store.has(id)\r\n  }\r\n  /**\r\n   * delete one message\r\n   * @param id\r\n   * @returns\r\n   */\r\n  async delete(id: string): Promise<boolean> {\r\n    return await this.#store.delete(id)\r\n  }\r\n  /**\r\n   * clear one conversation，it will be used when you set a new system prompt，which means that you will be in a new context，so early messages will be deleted\r\n   * @param id last conversation id\r\n   */\r\n  async clear1Conversation(id?: string) {\r\n    let parentMessageId: string | undefined = id\r\n    let cnt = 0\r\n    while (parentMessageId && cnt < this.#maxFindDepth) {\r\n      cnt++\r\n      const msg: TCommonMessage | undefined = await this.get(parentMessageId)\r\n      if (msg) {\r\n        await this.delete(msg.id)\r\n      }\r\n      parentMessageId = msg?.parentMessageId\r\n    }\r\n  }\r\n  /**\r\n   * find messages in a conversation by id\r\n   * @param id parentMessageId\r\n   */\r\n  async findMessages(opts: {\r\n    id: string | undefined\r\n    tokenizer: Tokenizer\r\n    limit: number\r\n    availableTokens: number\r\n    ignore: boolean\r\n  }) {\r\n    let {\r\n      id = undefined,\r\n      tokenizer,\r\n      limit,\r\n      availableTokens,\r\n      ignore = false,\r\n    } = opts\r\n    let parentMessageId: string | undefined = id\r\n    let cnt = 0\r\n    const messages: TChatGPTHTTPDataMessage[] = []\r\n    while (parentMessageId && cnt < this.#maxFindDepth) {\r\n      const msg: TCommonMessage | undefined = await this.#store.get(\r\n        parentMessageId,\r\n      )\r\n      if (msg && !(ignore && msg.role === ERole.assistant)) {\r\n        let tokensCnt = msg.tokens || tokenizer.getTokenCnt(msg.text)\r\n        if (tokensCnt <= limit) {\r\n          if (availableTokens < tokensCnt) break\r\n          messages.unshift({\r\n            role: msg.role,\r\n            content: msg.text,\r\n          })\r\n          cnt++\r\n          availableTokens -= tokensCnt\r\n        }\r\n      }\r\n      parentMessageId = msg?.parentMessageId\r\n    }\r\n    if (this.#debug) {\r\n      log('availableTokens', availableTokens)\r\n    }\r\n    return messages\r\n  }\r\n  /**\r\n   * clear the store\r\n   */\r\n  async clearAll() {\r\n    await this.#store.clear()\r\n  }\r\n}\r\n","import { v4 as uuid } from 'uuid'\r\nimport { stdin, stdout } from 'process'\r\nimport { createInterface } from 'readline'\r\nexport * from './log'\r\n\r\nexport function getReadLine() {\r\n  const rl = createInterface({ input: stdin, output: stdout })\r\n  const iter = rl[Symbol.asyncIterator]()\r\n  return async () => (await iter.next()).value\r\n}\r\n\r\n/**\r\n * generate unique id by uuidV4\r\n */\r\nexport function genId() {\r\n  return uuid()\r\n}\r\n","export function log(...args: any[]) {\r\n  console.log('----------------------------------')\r\n  console.log(...args)\r\n}\r\n","import { AxiosRequestConfig } from 'axios'\r\nimport { TiktokenEmbedding } from '@dqbd/tiktoken'\r\n/**\r\n * ChatCompletion 回答\r\n */\r\nexport interface IChatCompletion {\r\n  // {\r\n  //   \"id\": \"chatcmpl-6psB2OWQOgHwCWzTIsQMnhB3fst1J\",\r\n  //   \"object\": \"chat.completion\",\r\n  //   \"created\": 1677821004,\r\n  //   \"model\": \"gpt-3.5-turbo-0301\",\r\n  //   \"usage\": {\r\n  //     \"prompt_tokens\": 47,\r\n  //     \"completion_tokens\": 391,\r\n  //     \"total_tokens\": 438\r\n  //   },\r\n  //   \"choices\": [\r\n  //     {\r\n  //       \"message\": {\r\n  //         \"role\": \"assistant\",\r\n  //         \"content\": \"回答\"\r\n  //       },\r\n  //       \"finish_reason\": \"stop\",\r\n  //       \"index\": 0\r\n  //     }\r\n  //   ]\r\n  // }\r\n\r\n  id: string\r\n  object: string\r\n  created: number\r\n  model: string\r\n  usage: {\r\n    prompt_tokens: number\r\n    completion_tokens: number\r\n    total_tokens: number\r\n  }\r\n  choices: {\r\n    // content 即为答案\r\n    message: { role: string; content: string }\r\n    finish_reason: string\r\n    index: number\r\n  }[]\r\n}\r\n\r\n/**\r\n * response message\r\n */\r\nexport interface IChatGPTResponse {\r\n  id: string\r\n  text: string\r\n  created: number\r\n  role: ERole\r\n  parentMessageId?: string\r\n  tokens?: number\r\n}\r\n/**\r\n * user message\r\n */\r\nexport interface IChatGPTUserMessage {\r\n  id: string\r\n  text: string\r\n  role: ERole\r\n  parentMessageId?: string\r\n  tokens?: number\r\n}\r\n/**\r\n * system situation message\r\n */\r\nexport interface IChatGPTSystemMessage {\r\n  id: string\r\n  text: string\r\n  role: ERole\r\n  parentMessageId?: string\r\n  tokens?: number\r\n}\r\nexport type TChatGPTHTTPDataMessage = {\r\n  role: ERole\r\n  content: string\r\n}\r\n\r\nexport enum ERole {\r\n  /**\r\n   * conversation situation\r\n   */\r\n  system = 'system',\r\n  /**\r\n   * role is user\r\n   */\r\n  user = 'user',\r\n  /**\r\n   * role is chatgpt\r\n   */\r\n  assistant = 'assistant',\r\n}\r\n\r\nexport interface IConversationStoreParams {\r\n  maxKeys?: number\r\n  maxFindDepth?: number\r\n  debug?: boolean\r\n}\r\n\r\nexport interface IChatGPTParams {\r\n  /**\r\n   * apiKey, you can get it in https://platform.openai.com/account/api-keys,You can apply for up to 5 at most.\r\n   */\r\n  apiKey: string\r\n  /**\r\n   * model，default is 'gpt-3.5-turbo'\r\n   */\r\n  model?: string\r\n  /**\r\n   * print logs\r\n   */\r\n  debug?: boolean\r\n  /**\r\n   * axios configs\r\n   */\r\n  requestConfig?: AxiosRequestConfig\r\n  /**\r\n   * configs for store\r\n   */\r\n  storeConfig?: {\r\n    /**\r\n     * lru max keys, default `100000`\r\n     */\r\n    maxKeys?: number\r\n    /**\r\n     * Recursively search for historical messages, default `20` messages will be sent to the ChatGPT server\r\n     */\r\n    maxFindDepth?: number\r\n  }\r\n  tokenizerConfig?: ITokensParams\r\n  /**\r\n   * the maximum number of tokens when initiating a request, including prompts and completion. The default value is 4096.\r\n   */\r\n  maxTokens?: number\r\n  /**\r\n   * The maximum number of tokens for a single message. It is used to prevent from sending too many tokens to the ChatGPT server.\r\n   * If this number is exceeded, the message will be deleted and not passed on as a prompt to the chatGPT server. The default value is `1000`.\r\n   * - notice: **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**.\r\n   */\r\n  limitTokensInAMessage?: number\r\n  /**\r\n   * same reason as `limitTokensInAMessage`, **Maybe the message returned by ChatGPT should not be sent to the ChatGPT server as a prompt for the next conversation**, default value is `false` \r\n   * - `true`: will ignore ChatGPT server message in the next sendMessage, and will only refer to `limitTokensInAMessage` in history messages\r\n   * - `false`: will only refer to `limitTokensInAMessage` in history messages\r\n   */\r\n  ignoreServerMessagesInPrompt?: boolean\r\n}\r\n\r\n/**\r\n * Tokenizer params\r\n */\r\nexport interface ITokensParams {\r\n  /**\r\n   * \"gpt2\" | \"r50k_base\" | \"p50k_base\" | \"p50k_edit\" | \"cl100k_base\", default 'cl100k_base'\r\n   */\r\n  encoding?: TiktokenEmbedding\r\n\r\n  /**\r\n   * replace regexp\r\n   */\r\n  replaceReg?: RegExp\r\n  /**\r\n   * replace function\r\n   */\r\n  replaceCallback?: (...args: any[]) => string\r\n}\r\n\r\nexport type TCommonMessage =\r\n  | IChatGPTResponse\r\n  | IChatGPTUserMessage\r\n  | IChatGPTSystemMessage","import { get_encoding, Tiktoken } from '@dqbd/tiktoken'\r\nimport { ITokensParams, TCommonMessage } from './types'\r\n\r\nexport default class Tokenizer {\r\n  #tokenizer: Tiktoken\r\n  #replaceReg: RegExp\r\n  #replaceCallback: (...args: any[]) => string\r\n  constructor(opts: ITokensParams) {\r\n    const {\r\n      encoding = 'cl100k_base',\r\n      replaceReg = /<\\|endoftext\\|>/g,\r\n      replaceCallback = (...args: any[]) => '',\r\n    } = opts\r\n    this.#tokenizer = get_encoding(encoding)\r\n    this.#replaceReg = replaceReg\r\n    this.#replaceCallback = replaceCallback\r\n  }\r\n  #encode(text: string): Uint32Array {\r\n    return this.#tokenizer.encode(text)\r\n  }\r\n  /**\r\n   * get the text tokens count\r\n   * @param text\r\n   * @returns\r\n   */\r\n  getTokenCnt(msg: TCommonMessage | string){\r\n    if(typeof msg === 'object' && msg.tokens) return msg.tokens\r\n    msg = typeof msg === 'object' ? msg.text : msg\r\n    const text = msg.replace(this.#replaceReg, this.#replaceCallback)\r\n    return this.#encode(text).length\r\n  }\r\n}\r\n\r\n// const token = new Tokenizer({\r\n//   replaceReg: /hello|world/g,\r\n//   replaceCallback(c: string) {\r\n//     return 'ok'\r\n//   },\r\n// })\r\n// console.log(token.getTokenCnt('hello world'))\r\n","import axios from 'axios'\nimport { RawAxiosRequestConfig } from 'axios'\nimport { log } from './index'\n\ninterface IRequestOpts {\n  debug?: boolean\n}\n\nexport async function get(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n  const ins = axios.create({\n    method: 'GET',\n  })\n  if (opts.debug) {\n    ins.interceptors.request.use((config) => {\n      log('axios config', config)\n      return config\n    })\n  }\n  return (await ins({ ...config })).data\n}\nexport async function post(config: RawAxiosRequestConfig, opts: IRequestOpts) {\n  const ins = axios.create({\n    method: 'POST',\n  })\n  if (opts.debug) {\n    ins.interceptors.request.use((config) => {\n      log('axios config', config)\n      return config\n    })\n  }\n  return (await ins({ ...config })).data\n}\n","/**\n * docs https://platform.openai.com/docs/api-reference/chat\n */\nconst urls = {\n  listModels: 'https://api.openai.com/v1/models', // get\n  createCompletion: 'https://api.openai.com/v1/completions', // post\n  createChatCompletion: 'https://api.openai.com/v1/chat/completions' // post\n}\n\nexport default urls\n","import { AxiosRequestConfig } from 'axios'\n\nimport ConversationStore from './ConversationStore'\nimport Tokenizer from './Tokenizer'\nimport {\n  IChatCompletion,\n  IChatGPTResponse,\n  IChatGPTUserMessage,\n  IChatGPTSystemMessage,\n  ERole,\n  TChatGPTHTTPDataMessage,\n  IChatGPTParams,\n} from './types'\nimport { post } from './utils/request'\nimport URLS from './utils/urls'\nimport { genId, log } from './utils'\n\n// https://platform.openai.com/docs/api-reference/chat\n// curl https://api.openai.com/v1/chat/completions \\\n//   -H 'Content-Type: application/json' \\\n//   -H 'Authorization: Bearer YOUR_API_KEY' \\\n//   -d '{\n//   \"model\": \"gpt-3.5-turbo\",\n//   \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n// }'\n\n// role https://platform.openai.com/docs/guides/chat/introduction\nexport class ChatGPT {\n  #apiKey = ''\n  #model = ''\n  #urls = URLS\n  #debug = false\n  #requestConfig: AxiosRequestConfig\n  #store: ConversationStore\n  #tokenizer: Tokenizer\n  #maxTokens: number\n  #limitTokensInAMessage: number\n  #ignoreServerMessagesInPrompt: boolean\n  constructor(opts: IChatGPTParams) {\n    const {\n      apiKey,\n      model = 'gpt-3.5-turbo',\n      debug = false,\n      requestConfig = {},\n      storeConfig = {},\n      tokenizerConfig = {},\n      maxTokens = 4096,\n      limitTokensInAMessage = 1000,\n      ignoreServerMessagesInPrompt = false,\n    } = opts\n\n    this.#apiKey = apiKey\n    this.#model = model\n    this.#debug = debug\n    this.#requestConfig = requestConfig\n    this.#store = new ConversationStore({\n      ...storeConfig,\n      debug: this.#debug,\n    })\n    this.#tokenizer = new Tokenizer(tokenizerConfig)\n    this.#maxTokens = maxTokens\n    this.#limitTokensInAMessage = limitTokensInAMessage\n    this.#ignoreServerMessagesInPrompt = ignoreServerMessagesInPrompt\n  }\n\n  /**\n   * send message to ChatGPT server\n   * @param opts.text new message\n   * @param opts.systemPrompt prompt message\n   * @param opts.parentMessageId\n   */\n  sendMessage(\n    opts:\n      | {\n          text: string\n          systemPrompt?: string\n          parentMessageId?: string\n          onProgress?: (t: string) => void\n        }\n      | string,\n  ) {\n    return new Promise<IChatGPTResponse>(async (resolve, reject) => {\n      opts = typeof opts === 'string' ? { text: opts } : opts\n      let {\n        text,\n        systemPrompt = undefined,\n        parentMessageId = undefined,\n        onProgress = false,\n      } = opts\n      if (systemPrompt) {\n        if (parentMessageId)\n          await this.#store.clear1Conversation(parentMessageId)\n        parentMessageId = undefined\n      }\n      const model = this.#model\n      const userMessage: IChatGPTUserMessage = {\n        id: genId(),\n        text,\n        role: ERole.user,\n        parentMessageId,\n        tokens: this.#tokenizer.getTokenCnt(text),\n      }\n      const messages = await this.#makeConversations(userMessage, systemPrompt)\n      if (this.#debug) {\n        log('messages', messages)\n      }\n      if (onProgress) {\n        const stream = await post(\n          {\n            url: this.#urls.createChatCompletion,\n            ...this.#requestConfig,\n            headers: {\n              Authorization: this.#genAuthorization(),\n              'Content-Type': 'application/json',\n              ...{ ...(this.#requestConfig.headers || {}) },\n            },\n            data: {\n              stream: true,\n              model,\n              messages,\n              ...{ ...(this.#requestConfig.data || {}) },\n            },\n            responseType: 'stream',\n          },\n          {\n            debug: this.#debug,\n          },\n        )\n        const response: IChatGPTResponse = {\n          id: genId(),\n          text: '',\n          created: Math.floor(Date.now() / 1000),\n          role: ERole.assistant,\n          parentMessageId: userMessage.id,\n          tokens: 0,\n        }\n        stream.on('data', (buf: any) => {\n          try {\n            const dataArr = buf.toString().split('\\n')\n            let onDataPieceText = ''\n            for (const dataStr of dataArr) {\n              // split 之后的空行，或者结束通知\n              if (dataStr.indexOf('data: ') !== 0 || dataStr === 'data: [DONE]')\n                continue\n              const parsedData = JSON.parse(dataStr.slice(6)) // [data: ]\n              const pieceText = parsedData.choices[0].delta.content || ''\n              onDataPieceText += pieceText\n              // if (pieceText === '') {\n              //   console.log('[empty pieceText]', dataStr)\n              // }\n            }\n            if (typeof onProgress === 'function') {\n              onProgress(onDataPieceText)\n            }\n            response.text += onDataPieceText\n          } catch (e) {\n            log('[chatgpt api err]', e)\n          }\n        })\n\n        stream.on('end', () => {\n          response.tokens = this.#tokenizer.getTokenCnt(response.text)\n          resolve(response)\n        })\n      } else {\n        const res = (await post(\n          {\n            url: this.#urls.createChatCompletion,\n            ...this.#requestConfig,\n            headers: {\n              Authorization: this.#genAuthorization(),\n              'Content-Type': 'application/json',\n              ...{ ...(this.#requestConfig.headers || {}) },\n            },\n            data: {\n              model,\n              messages,\n              ...{ ...(this.#requestConfig.data || {}) },\n            },\n          },\n          {\n            debug: this.#debug,\n          },\n        )) as IChatCompletion\n        if (this.#debug) {\n          // log response\n          log(\n            'response',\n            JSON.stringify({\n              ...res,\n              choices: [],\n            }),\n          )\n        }\n        const response: IChatGPTResponse = {\n          id: genId(),\n          text: res?.choices[0]?.message?.content,\n          created: res.created,\n          role: ERole.assistant,\n          parentMessageId: userMessage.id,\n          tokens: res?.usage?.completion_tokens,\n        }\n        const msgsToBeStored = [userMessage, response]\n        if (systemPrompt) {\n          const systemMessage: IChatGPTSystemMessage = {\n            id: genId(),\n            text: systemPrompt,\n            role: ERole.system,\n            tokens: this.#tokenizer.getTokenCnt(systemPrompt),\n          }\n          userMessage.parentMessageId = systemMessage.id\n          msgsToBeStored.unshift(systemMessage)\n        }\n        await this.#store.set(msgsToBeStored)\n        resolve(response)\n      }\n    })\n  }\n\n  /**\n   * make conversations for http request data.messages\n   */\n  async #makeConversations(userMessage: IChatGPTUserMessage, prompt?: string) {\n    let messages: TChatGPTHTTPDataMessage[] = []\n    let usedTokens = this.#tokenizer.getTokenCnt(userMessage.text)\n    if (prompt) {\n      messages.push({\n        role: ERole.system,\n        content: prompt,\n      })\n    } else {\n      messages = await this.#store.findMessages({\n        id: userMessage.parentMessageId,\n        tokenizer: this.#tokenizer,\n        limit: this.#limitTokensInAMessage,\n        availableTokens: this.#maxTokens - usedTokens,\n        ignore: this.#ignoreServerMessagesInPrompt,\n      })\n    }\n    messages.push({\n      role: ERole.user,\n      content: userMessage.text,\n    })\n    return messages\n  }\n\n  async clear1Conversation(parentMessageId?: string) {\n    return await this.#store.clear1Conversation(parentMessageId)\n  }\n\n  /**\n   * generate HTTP Authorization\n   * @returns\n   */\n  #genAuthorization() {\n    return `Bearer ${this.#apiKey}`\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAA,kBAAiB;AACjB,uBAAqB;;;ACDrB,kBAA2B;;;ACApB,SAAS,OAAO,MAAa;AAClC,UAAQ,IAAI,oCAAoC;AAChD,UAAQ,IAAI,GAAG,IAAI;AACrB;;;ADWO,SAAS,QAAQ;AACtB,aAAO,YAAAA,IAAK;AACd;;;AEiEO,IAAK,QAAL,kBAAKC,WAAL;AAIL,EAAAA,OAAA,YAAS;AAIT,EAAAA,OAAA,UAAO;AAIP,EAAAA,OAAA,eAAY;AAZF,SAAAA;AAAA,GAAA;;;AHjFZ;AAeA,IAAqB,oBAArB,MAAuC;AAAA,EAQrC,YAAY,QAAkC;AAP9C;AACA;AAIA;AAAA;AAAA;AAAA;AACA;AAEE,UAAM,EAAE,UAAU,KAAQ,eAAe,IAAI,QAAQ,MAAM,IAAI;AAC/D,uBAAK,MAAO,IAAI,iBAAAC,QAAiC;AAAA,MAC/C,KAAK;AAAA,IACP,CAAC;AACD,uBAAK,QAAS,IAAI,YAAAC,QAA0B;AAAA,MAC1C,OAAO,mBAAK;AAAA,IACd,CAAC;AACD,uBAAK,eAAgB;AACrB,uBAAK,QAAS;AAEd,QAAI,mBAAK;AAAQ,UAAI,4BAA4B,MAAM;AAAA,EACzD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAAiD;AACzD,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,MAAwB;AAChC,eAAW,OAAO,MAAM;AACtB,YAAM,mBAAK,QAAO,IAAI,IAAI,IAAI,GAAG;AAAA,IACnC;AACA,QAAI,mBAAK;AAAQ,UAAI,YAAY,mBAAK,MAAK,IAAI;AAAA,EACjD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,IAAI,IAA8B;AACtC,WAAO,MAAM,mBAAK,QAAO,IAAI,EAAE;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,MAAM,OAAO,IAA8B;AACzC,WAAO,MAAM,mBAAK,QAAO,OAAO,EAAE;AAAA,EACpC;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAmB,IAAa;AACpC,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD;AACA,YAAM,MAAkC,MAAM,KAAK,IAAI,eAAe;AACtE,UAAI,KAAK;AACP,cAAM,KAAK,OAAO,IAAI,EAAE;AAAA,MAC1B;AACA,wBAAkB,2BAAK;AAAA,IACzB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,MAMhB;AACD,QAAI;AAAA,MACF,KAAK;AAAA,MACL;AAAA,MACA;AAAA,MACA;AAAA,MACA,SAAS;AAAA,IACX,IAAI;AACJ,QAAI,kBAAsC;AAC1C,QAAI,MAAM;AACV,UAAM,WAAsC,CAAC;AAC7C,WAAO,mBAAmB,MAAM,mBAAK,gBAAe;AAClD,YAAM,MAAkC,MAAM,mBAAK,QAAO;AAAA,QACxD;AAAA,MACF;AACA,UAAI,OAAO,EAAE,UAAU,IAAI,uCAA2B;AACpD,YAAI,YAAY,IAAI,UAAU,UAAU,YAAY,IAAI,IAAI;AAC5D,YAAI,aAAa,OAAO;AACtB,cAAI,kBAAkB;AAAW;AACjC,mBAAS,QAAQ;AAAA,YACf,MAAM,IAAI;AAAA,YACV,SAAS,IAAI;AAAA,UACf,CAAC;AACD;AACA,6BAAmB;AAAA,QACrB;AAAA,MACF;AACA,wBAAkB,2BAAK;AAAA,IACzB;AACA,QAAI,mBAAK,SAAQ;AACf,UAAI,mBAAmB,eAAe;AAAA,IACxC;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAIA,MAAM,WAAW;AACf,UAAM,mBAAK,QAAO,MAAM;AAAA,EAC1B;AACF;AAzHE;AACA;AAIA;AACA;;;AItBF,sBAAuC;AAAvC;AAGA,IAAqB,YAArB,MAA+B;AAAA,EAI7B,YAAY,MAAqB;AAUjC;AAbA;AACA;AACA;AAEE,UAAM;AAAA,MACJ,WAAW;AAAA,MACX,aAAa;AAAA,MACb,kBAAkB,IAAI,SAAgB;AAAA,IACxC,IAAI;AACJ,uBAAK,gBAAa,8BAAa,QAAQ;AACvC,uBAAK,aAAc;AACnB,uBAAK,kBAAmB;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,YAAY,KAA6B;AACvC,QAAG,OAAO,QAAQ,YAAY,IAAI;AAAQ,aAAO,IAAI;AACrD,UAAM,OAAO,QAAQ,WAAW,IAAI,OAAO;AAC3C,UAAM,OAAO,IAAI,QAAQ,mBAAK,cAAa,mBAAK,iBAAgB;AAChE,WAAO,sBAAK,oBAAL,WAAa,MAAM;AAAA,EAC5B;AACF;AA3BE;AACA;AACA;AAWA;AAAA,YAAO,SAAC,MAA2B;AACjC,SAAO,mBAAK,YAAW,OAAO,IAAI;AACpC;;;ACnBF,mBAAkB;AAoBlB,eAAsB,KAAK,QAA+B,MAAoB;AAC5E,QAAM,MAAM,aAAAC,QAAM,OAAO;AAAA,IACvB,QAAQ;AAAA,EACV,CAAC;AACD,MAAI,KAAK,OAAO;AACd,QAAI,aAAa,QAAQ,IAAI,CAACC,YAAW;AACvC,UAAI,gBAAgBA,OAAM;AAC1B,aAAOA;AAAA,IACT,CAAC;AAAA,EACH;AACA,UAAQ,MAAM,IAAI,EAAE,GAAG,OAAO,CAAC,GAAG;AACpC;;;AC5BA,IAAM,OAAO;AAAA,EACX,YAAY;AAAA;AAAA,EACZ,kBAAkB;AAAA;AAAA,EAClB,sBAAsB;AAAA;AACxB;AAEA,IAAO,eAAQ;;;ACTf,4BAAAC,SAAA,gBAAAC,SAAAC,aAAA;AA2BO,IAAM,UAAN,MAAc;AAAA,EAWnB,YAAY,MAAsB;AAwLlC;AAAA;AAAA;AAAA,uBAAM;AAgCN;AAAA;AAAA;AAAA;AAAA;AAlOA,gCAAU;AACV,+BAAS;AACT,8BAAQ;AACR,uBAAAF,SAAS;AACT;AACA,uBAAAC,SAAA;AACA,uBAAAC,aAAA;AACA;AACA;AACA;AAEE,UAAM;AAAA,MACJ;AAAA,MACA,QAAQ;AAAA,MACR,QAAQ;AAAA,MACR,gBAAgB,CAAC;AAAA,MACjB,cAAc,CAAC;AAAA,MACf,kBAAkB,CAAC;AAAA,MACnB,YAAY;AAAA,MACZ,wBAAwB;AAAA,MACxB,+BAA+B;AAAA,IACjC,IAAI;AAEJ,uBAAK,SAAU;AACf,uBAAK,QAAS;AACd,uBAAKF,SAAS;AACd,uBAAK,gBAAiB;AACtB,uBAAKC,SAAS,IAAI,kBAAkB;AAAA,MAClC,GAAG;AAAA,MACH,OAAO,mBAAKD;AAAA,IACd,CAAC;AACD,uBAAKE,aAAa,IAAI,UAAU,eAAe;AAC/C,uBAAK,YAAa;AAClB,uBAAK,wBAAyB;AAC9B,uBAAK,+BAAgC;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,YACE,MAQA;AACA,WAAO,IAAI,QAA0B,OAAO,SAAS,WAAW;AAjFpE;AAkFM,aAAO,OAAO,SAAS,WAAW,EAAE,MAAM,KAAK,IAAI;AACnD,UAAI;AAAA,QACF;AAAA,QACA,eAAe;AAAA,QACf,kBAAkB;AAAA,QAClB,aAAa;AAAA,MACf,IAAI;AACJ,UAAI,cAAc;AAChB,YAAI;AACF,gBAAM,mBAAKD,SAAO,mBAAmB,eAAe;AACtD,0BAAkB;AAAA,MACpB;AACA,YAAM,QAAQ,mBAAK;AACnB,YAAM,cAAmC;AAAA,QACvC,IAAI,MAAM;AAAA,QACV;AAAA,QACA;AAAA,QACA;AAAA,QACA,QAAQ,mBAAKC,aAAW,YAAY,IAAI;AAAA,MAC1C;AACA,YAAM,WAAW,MAAM,sBAAK,0CAAL,WAAwB,aAAa;AAC5D,UAAI,mBAAKF,UAAQ;AACf,YAAI,YAAY,QAAQ;AAAA,MAC1B;AACA,UAAI,YAAY;AACd,cAAM,SAAS,MAAM;AAAA,UACnB;AAAA,YACE,KAAK,mBAAK,OAAM;AAAA,YAChB,GAAG,mBAAK;AAAA,YACR,SAAS;AAAA,cACP,eAAe,sBAAK,wCAAL;AAAA,cACf,gBAAgB;AAAA,cAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,YAC9C;AAAA,YACA,MAAM;AAAA,cACJ,QAAQ;AAAA,cACR;AAAA,cACA;AAAA,cACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,YAC3C;AAAA,YACA,cAAc;AAAA,UAChB;AAAA,UACA;AAAA,YACE,OAAO,mBAAKA;AAAA,UACd;AAAA,QACF;AACA,cAAM,WAA6B;AAAA,UACjC,IAAI,MAAM;AAAA,UACV,MAAM;AAAA,UACN,SAAS,KAAK,MAAM,KAAK,IAAI,IAAI,GAAI;AAAA,UACrC;AAAA,UACA,iBAAiB,YAAY;AAAA,UAC7B,QAAQ;AAAA,QACV;AACA,eAAO,GAAG,QAAQ,CAAC,QAAa;AAC9B,cAAI;AACF,kBAAM,UAAU,IAAI,SAAS,EAAE,MAAM,IAAI;AACzC,gBAAI,kBAAkB;AACtB,uBAAW,WAAW,SAAS;AAE7B,kBAAI,QAAQ,QAAQ,QAAQ,MAAM,KAAK,YAAY;AACjD;AACF,oBAAM,aAAa,KAAK,MAAM,QAAQ,MAAM,CAAC,CAAC;AAC9C,oBAAM,YAAY,WAAW,QAAQ,CAAC,EAAE,MAAM,WAAW;AACzD,iCAAmB;AAAA,YAIrB;AACA,gBAAI,OAAO,eAAe,YAAY;AACpC,yBAAW,eAAe;AAAA,YAC5B;AACA,qBAAS,QAAQ;AAAA,UACnB,SAAS,GAAP;AACA,gBAAI,qBAAqB,CAAC;AAAA,UAC5B;AAAA,QACF,CAAC;AAED,eAAO,GAAG,OAAO,MAAM;AACrB,mBAAS,SAAS,mBAAKE,aAAW,YAAY,SAAS,IAAI;AAC3D,kBAAQ,QAAQ;AAAA,QAClB,CAAC;AAAA,MACH,OAAO;AACL,cAAM,MAAO,MAAM;AAAA,UACjB;AAAA,YACE,KAAK,mBAAK,OAAM;AAAA,YAChB,GAAG,mBAAK;AAAA,YACR,SAAS;AAAA,cACP,eAAe,sBAAK,wCAAL;AAAA,cACf,gBAAgB;AAAA,cAChB,GAAG,EAAE,GAAI,mBAAK,gBAAe,WAAW,CAAC,EAAG;AAAA,YAC9C;AAAA,YACA,MAAM;AAAA,cACJ;AAAA,cACA;AAAA,cACA,GAAG,EAAE,GAAI,mBAAK,gBAAe,QAAQ,CAAC,EAAG;AAAA,YAC3C;AAAA,UACF;AAAA,UACA;AAAA,YACE,OAAO,mBAAKF;AAAA,UACd;AAAA,QACF;AACA,YAAI,mBAAKA,UAAQ;AAEf;AAAA,YACE;AAAA,YACA,KAAK,UAAU;AAAA,cACb,GAAG;AAAA,cACH,SAAS,CAAC;AAAA,YACZ,CAAC;AAAA,UACH;AAAA,QACF;AACA,cAAM,WAA6B;AAAA,UACjC,IAAI,MAAM;AAAA,UACV,OAAM,sCAAK,QAAQ,OAAb,mBAAiB,YAAjB,mBAA0B;AAAA,UAChC,SAAS,IAAI;AAAA,UACb;AAAA,UACA,iBAAiB,YAAY;AAAA,UAC7B,SAAQ,gCAAK,UAAL,mBAAY;AAAA,QACtB;AACA,cAAM,iBAAiB,CAAC,aAAa,QAAQ;AAC7C,YAAI,cAAc;AAChB,gBAAM,gBAAuC;AAAA,YAC3C,IAAI,MAAM;AAAA,YACV,MAAM;AAAA,YACN;AAAA,YACA,QAAQ,mBAAKE,aAAW,YAAY,YAAY;AAAA,UAClD;AACA,sBAAY,kBAAkB,cAAc;AAC5C,yBAAe,QAAQ,aAAa;AAAA,QACtC;AACA,cAAM,mBAAKD,SAAO,IAAI,cAAc;AACpC,gBAAQ,QAAQ;AAAA,MAClB;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EA6BA,MAAM,mBAAmB,iBAA0B;AACjD,WAAO,MAAM,mBAAKA,SAAO,mBAAmB,eAAe;AAAA,EAC7D;AASF;AArOE;AACA;AACA;AACAD,UAAA;AACA;AACAC,UAAA;AACAC,cAAA;AACA;AACA;AACA;AAyLM;AAAA,uBAAkB,eAAC,aAAkC,QAAiB;AAC1E,MAAI,WAAsC,CAAC;AAC3C,MAAI,aAAa,mBAAKA,aAAW,YAAY,YAAY,IAAI;AAC7D,MAAI,QAAQ;AACV,aAAS,KAAK;AAAA,MACZ;AAAA,MACA,SAAS;AAAA,IACX,CAAC;AAAA,EACH,OAAO;AACL,eAAW,MAAM,mBAAKD,SAAO,aAAa;AAAA,MACxC,IAAI,YAAY;AAAA,MAChB,WAAW,mBAAKC;AAAA,MAChB,OAAO,mBAAK;AAAA,MACZ,iBAAiB,mBAAK,cAAa;AAAA,MACnC,QAAQ,mBAAK;AAAA,IACf,CAAC;AAAA,EACH;AACA,WAAS,KAAK;AAAA,IACZ;AAAA,IACA,SAAS,YAAY;AAAA,EACvB,CAAC;AACD,SAAO;AACT;AAUA;AAAA,sBAAiB,WAAG;AAClB,SAAO,UAAU,mBAAK;AACxB;","names":["uuid","ERole","LRUCache","Keyv","axios","config","_debug","_store","_tokenizer"]}